{"cells": [{"metadata": {"trusted": true}, "cell_type": "code", "source": "## Import packages\nimport sys\nimport boto3 \nimport pandas as pd\nimport numpy as np\nimport s3fs\nimport StringIO\nfrom uszipcode import SearchEngine\nfrom uszipcode import Zipcode\nfrom datetime import datetime, timedelta", "execution_count": 1, "outputs": [{"output_type": "display_data", "data": {"text/plain": "VBox()", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "9bb04818c5b5416caf3f667d697b5058"}}, "metadata": {}}, {"output_type": "stream", "text": "Starting Spark application\n", "name": "stdout"}, {"output_type": "display_data", "data": {"text/plain": "<IPython.core.display.HTML object>", "text/html": "<table>\n<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>9</td><td>application_1544060771622_0010</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-172-31-20-170.us-west-1.compute.internal:20888/proxy/application_1544060771622_0010/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-172-31-25-165.us-west-1.compute.internal:8042/node/containerlogs/container_1544060771622_0010_01_000001/livy\">Link</a></td><td>\u2714</td></tr></table>"}, "metadata": {}}, {"output_type": "stream", "text": "SparkSession available as 'spark'.\n", "name": "stdout"}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "zipcodeMap = {1: 7114, 2: 11430, 3: 10469, 4: 10009, 5: 10309, 6: 10305, 7: 11101, 8: 11105, 9: 11358, 10: 11434, 11: 11214, 12: 10280, 13: 10280, 14: 11209, 15: 11359, 16: 11361, 17: 10506, 18: 10458, 19: 11426, 20: 14813, 21: 11206, 22: 11207, 23: 10314, 24: 12913, 25: 11201, 26: 11212, 27: 11697, 28: 11435, 29: 11235, 30: 11693, 31: 10453, 32: 10462, 33: 11201, 34: 11251, 35: 11212, 36: 11221, 37: 11237, 38: 11411, 39: 11234, 40: 11231, 41: 10026, 42: 10027, 43: 10019, 44: 10309, 45: 10013, 46: 10464, 47: 10457, 48: 10001, 49: 11205, 50: 10002, 51: 10475, 52: 11201, 53: 11356, 54: 11231, 55: 11224, 56: 11368, 57: 11368, 58: 10465, 59: 10457, 60: 10459, 61: 11238, 62: 11205, 63: 11208, 64: 11363, 65: 11201, 66: 11201, 67: 11228, 68: 10019, 69: 10451, 70: 11369, 71: 11203, 72: 11236, 73: 11355, 74: 10035, 75: 10029, 76: 10003, 77: 11207, 78: 10457, 79: 10211, 80: 11211, 81: 10466, 82: 11373, 83: 11378, 84: 10308, 85: 11226, 86: 11691, 87: 10005, 88: 10006, 89: 11226, 90: 10010, 91: 11239, 92: 11355, 93: 11368, 94: 10468, 95: 11375, 96: 11385, 97: 11205, 98: 11365, 99: 10312, 100: 10018, 101: 11004, 102: 11385, 103: 10012, 104: 10012, 105: 10012, 106: 11215, 107: 10016, 108: 11223, 109: 10308, 110: 10306, 111: 11232, 112: 11222, 113: 10012, 114: 10013, 115: 10301, 116: 10031, 117: 11692, 118: 10314, 119: 10452, 120: 10034, 121: 11366, 122: 11423, 123: 11229, 124: 11414, 125: 10006, 126: 10474, 127: 10034, 128: 10031, 129: 11372, 130: 11412, 131: 11423, 132: 11430, 133: 11218, 134: 11415, 135: 11367, 136: 10468, 137: 10016, 138: 11371, 139: 11413, 140: 10021, 141: 10065, 142: 10023, 143: 10024, 144: 10013, 145: 11101, 146: 11101, 147: 10459, 148: 10009, 149: 11201, 150: 11235, 151: 10025, 152: 10027, 153: 10463, 154: 11234, 155: 11234, 156: 10303, 157: 11378, 158: 10014, 159: 10456, 160: 11379, 161: 10018, 162: 10022, 163: 10018, 164: 10017, 165: 11230, 166: 10027, 167: 10456, 168: 10451, 169: 10453, 170: 10010, 171: 11354, 172: 10306, 173: 11368, 174: 10467, 175: 11364, 176: 10306, 177: 11233, 178: 11230, 179: 11103, 180: 11416, 181: 11215, 182: 10462, 183: 10461, 184: 10461, 185: 10461, 186: 10016, 187: 10302, 188: 11225, 189: 11238, 190: 11215, 191: 11427, 192: 11355, 193: 11101, 194: 10035, 195: 11231, 196: 11374, 197: 11418, 198: 11385, 199: 11370, 200: 10471, 201: 11694, 202: 10044, 203: 11422, 204: 10309, 205: 11412, 206: 11378, 207: 11370, 208: 10469, 209: 10013, 210: 11235, 211: 10013, 212: 10472, 213: 10472, 214: 10305, 215: 11435, 216: 11420, 217: 11221, 218: 11413, 219: 11413, 220: 10463, 221: 10304, 222: 11239, 223: 11105, 224: 10009, 225: 11205, 226: 11104, 227: 11211, 228: 11232, 229: 10022, 230: 10036, 231: 10007, 232: 10002, 233: 10022, 234: 10003, 235: 10453, 236: 10021, 237: 10028, 238: 10044, 239: 10065, 240: 10463, 241: 10463, 242: 10461, 243: 10032, 244: 10034, 245: 10310, 246: 10036, 247: 10451, 248: 10460, 249: 10014, 250: 10462, 251: 10314, 252: 11357, 253: 11357, 254: 10467, 255: 10467, 256: 10467, 257: 11215, 258: 11421, 259: 10470, 260: 11377, 261: 10048, 262: 10028, 263: 10028, 264: 0, 265: 0}", "execution_count": 2, "outputs": [{"output_type": "display_data", "data": {"text/plain": "VBox()", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "dc4ed9c1af5e45029bebf14c808c26c5"}}, "metadata": {}}]}, {"metadata": {}, "cell_type": "markdown", "source": "## Read file and extract a map of required columns"}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "def readfile(filename, row_count=None):\n    # datafile=\"s3n://nyc-tlc/trip data/green_tripdata_2018-06.csv\" # has area codes\n    # datafile=\"s3n://nyc-tlc/trip data/yellow_tripdata_2015-07.csv\" # has lat/long data\n    df = pd.read_csv(filename)\n    dfcolumns = pd.read_csv(filename, nrows = 1)\n    ncols = len(dfcolumns.columns)\n    if row_count:\n        df = pd.read_csv(filename, header = None, sep= ',', \n                     skiprows = 1, usecols = list(range(ncols)),\n                     names = dfcolumns.columns, low_memory=False, nrows=row_count)\n    else:\n        df = pd.read_csv(filename, header = None, sep= ',', \n                         skiprows = 1, usecols = list(range(ncols)),\n                         names = dfcolumns.columns, low_memory=False)\n    \n    column_map = {}\n    useful_columns = [\"pickup_longitude\", \"pickup_latitude\", \"dropoff_longitude\", \"dropoff_latitude\", \"pickup_datetime\", \"dropoff_datetime\", \"dolocationid\", \"pulocationid\", \"fare_amount\", \"trip_distance\", \"passenger_count\"]\n    col_names = set(df.columns)\n    drop_columns = []\n    for c in col_names:\n        cl = c.lower()\n        for u in useful_columns:\n            if cl in u or u in cl:\n                column_map[c] = u\n        if c not in column_map:\n            drop_columns.append(c)\n#     print(drop_columns)\n    df = df.drop(drop_columns, axis=1)\n\n    df.rename(columns=column_map, inplace=True)\n    return df", "execution_count": 3, "outputs": [{"output_type": "display_data", "data": {"text/plain": "VBox()", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "4a4399155317441faf5f483635ed466d"}}, "metadata": {}}]}, {"metadata": {}, "cell_type": "markdown", "source": "## Sanity check - all the required columns should be present"}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "def sanityCheck(columns):\n    required_columns = [\"pickup_datetime\", \"dropoff_datetime\", \"fare_amount\", \"trip_distance\", \"passenger_count\"]\n    for c in required_columns:\n        if c not in columns:\n            print(\"Required column {} not found in the data. Exiting.\".format(c))\n            return False\n\n    exit = False\n    for c in [\"pickup_longitude\", \"pickup_latitude\", \"dropoff_longitude\", \"dropoff_latitude\"]:\n        if c not in columns:\n            exit = True\n\n    if exit:\n        for c in [\"dolocationid\", \"pulocationid\"]:\n            if c not in columns:\n                print(\"Required columns for location not found in the data. Exiting.\".format(c))\n                return False\n\n    print(\"Sanity check complete. Data can be preprocessed.\")\n    return True", "execution_count": 4, "outputs": [{"output_type": "display_data", "data": {"text/plain": "VBox()", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "a8d4611a709b4439ae0ef8391d718174"}}, "metadata": {}}]}, {"metadata": {}, "cell_type": "markdown", "source": "## Feature engineering - add zipcode"}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "search = SearchEngine(simple_zipcode=True)\ndef get_zip_code(latitude,longitude):\n    try:\n        return 0\n        result = search.by_coordinates(latitude, longitude, radius=5, returns=1)\n        return result[0].zipcode\n    except Exception as e:\n        return '10001'\n\ndef applyZipCode(lat, lng):\n    zipcode = get_zip_code(lat, lng)\n    return zipcode\n\ndef get_loc_id_zipcode(loc_id):\n    return zipcodeMap[loc_id]\n\ndef generate_zipcode_columns(df):\n    if 'pickup_latitude' in df.columns:\n#         df['pickup_zipcode'] = df.apply(lambda row: applyZipCode(row.pickup_latitude, row.pickup_longitude), axis=1)\n#         df['dropoff_zipcode'] = df.apply(lambda row: applyZipCode(row.dropoff_latitude, row.dropoff_longitude), axis=1)\n        df.drop([\"pickup_longitude\",\"pickup_latitude\", \"dropoff_longitude\", \"dropoff_latitude\"], axis=1, inplace=True)\n    else:\n        df['pickup_zipcode'] = df.apply(lambda row: get_loc_id_zipcode(row.pulocationid), axis=1)\n        df['dropoff_zipcode'] = df.apply(lambda row: get_loc_id_zipcode(row.dolocationid), axis=1)\n        df.drop([\"dolocationid\", \"pulocationid\"], axis=1, inplace=True)\n    return df\n\n# search = SearchEngine(simple_zipcode=True)\n# def get_zip_code_from_coords(coord):\n#     try:\n#         result = search.by_coordinates(coord[0], coord[1], radius=5, returns=1)\n#         return result[0].zipcode\n#     except Exception as e:\n#         return '10001'\n\n\n# def generate_zipcode_columns(df):\n#     if 'pickup_latitude' in df.columns:\n#         df_vals = df[['pickup_latitude', 'pickup_longitude', 'dropoff_latitude', 'dropoff_longitude']].values\n#         pickup = df_vals[:,[0,1]]\n#         dropoff = df_vals[:,[2,3]]\n#         df['pickup_zipcode'] = [search.by_coordinates(v[0], v[1], radius=5, returns=1) for v in pickup]\n#         df['dropoff_zipcode'] = [search.by_coordinates(v[0], v[1], radius=5, returns=1) for v in dropoff]\n# #         df['pickup_zipcode'] = np.apply_along_axis(get_zip_code_from_coords, 1, pickup)\n# #         df['dropoff_zipcode'] = np.apply_along_axis(get_zip_code_from_coords, 1, dropoff)\n#         df.drop([\"pickup_longitude\",\"pickup_latitude\", \"dropoff_longitude\", \"dropoff_latitude\"], axis=1, inplace=True)\n#     if 'dolocationid' in df.columns:\n#         df_vals = df[['pulocationid', 'dolocationid']].values\n#         pickup = df_vals[:,0]\n#         dropoff = df_vals[:,1]\n# #         df['pickup_zipcode'] = np.apply_along_axis(get_loc_id_zipcode, 1, pickup)\n# #         df['dropoff_zipcode'] = np.apply_along_axis(get_loc_id_zipcode, 1, dropoff)\n#         df['pickup_zipcode'] = [zipcodeMap[v] for v in pickup]\n#         df['dropoff_zipcode'] = [zipcodeMap[v] for v in dropoff]\n#         df.drop([\"dolocationid\", \"pulocationid\"], axis=1, inplace=True)\n#     return df", "execution_count": 15, "outputs": [{"output_type": "display_data", "data": {"text/plain": "VBox()", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "2809d973c7f343efb939b1f8dbc45304"}}, "metadata": {}}]}, {"metadata": {}, "cell_type": "markdown", "source": "## Remove NaN values"}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "def remove_nan_values(df):\n    df = df.replace(to_replace='None', value=np.nan).dropna()\n#     print(df.shape)\n    df = df[(df != 0).all(1)]\n#     print(df.shape)\n    return df", "execution_count": 16, "outputs": [{"output_type": "display_data", "data": {"text/plain": "VBox()", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "e1104c46acc442fa9374a83b11fe4de9"}}, "metadata": {}}]}, {"metadata": {}, "cell_type": "markdown", "source": "## Remove invalid latitude/longitude"}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "def remove_invalid_lat_long(df):\n    max_lat = 40.917577\n    min_lat = 40.477399 \n    max_long = -73.700272 \n    min_long = -74.259090\n    loc_cols = [\"pickup_longitude\", \"pickup_latitude\", \"dropoff_longitude\", \"dropoff_latitude\"]\n#     print(loc_cols)\n    lat_cols = []\n    long_cols = []\n    for column in loc_cols:\n        if \"latitude\" in column.lower():\n            lat_cols.append(column)\n        elif \"longitude\" in column.lower():\n            long_cols.append(column)\n\n    for col in lat_cols:\n        df = df.loc[(df[col] >= min_lat) & (df[col] <= max_lat)]\n#         print(df.shape)\n\n    for col in long_cols:\n        df = df.loc[(df[col] >= min_long) & (df[col] <= max_long)]\n#         print(df.shape)\n    \n    return df", "execution_count": 17, "outputs": [{"output_type": "display_data", "data": {"text/plain": "VBox()", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "db1f41a0cda248948dbc79291c86bfc7"}}, "metadata": {}}]}, {"metadata": {}, "cell_type": "markdown", "source": "## Remove trips with invalid fare"}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "def remove_invalid_fare_trips(df):\n    df = df.loc[(df['fare_amount'] >= 2.5)]\n    return df", "execution_count": 18, "outputs": [{"output_type": "display_data", "data": {"text/plain": "VBox()", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "bf66c608a537498cbcd393f3662fa86c"}}, "metadata": {}}]}, {"metadata": {}, "cell_type": "markdown", "source": "## Fix Column Datatypes"}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "def fix_column_datatypes(df):\n    date_columns = ['pickup_datetime', 'dropoff_datetime']\n    numeric_columns = ['passenger_count', 'trip_distance', 'fare_amount']\n    df[date_columns] = df[date_columns].apply(pd.to_datetime)\n    df[numeric_columns] = df[numeric_columns].apply(pd.to_numeric)\n    return df", "execution_count": 19, "outputs": [{"output_type": "display_data", "data": {"text/plain": "VBox()", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "c58849457fd149ed8665976feb712cff"}}, "metadata": {}}]}, {"metadata": {}, "cell_type": "markdown", "source": "## Generate Datetime Features"}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "def generate_datetime_features(df):\n#     df['pickup_date']= df['pickup_datetime'].dt.date\n    df['pickup_day']=df['pickup_datetime'].apply(lambda x:x.day)\n    df['pickup_hour']=df['pickup_datetime'].apply(lambda x:x.hour)\n    df['pickup_day_of_week']=df['pickup_datetime'].apply(lambda x:x.weekday())\n    df['pickup_month']=df['pickup_datetime'].apply(lambda x:x.month)\n    df['pickup_year']=df['pickup_datetime'].apply(lambda x:x.year)\n    \n#     df['dropoff_date']= df['dropoff_datetime'].dt.date\n#     df['dropoff_day']=df['dropoff_datetime'].apply(lambda x:x.day)\n#     df['dropoff_hour']=df['dropoff_datetime'].apply(lambda x:x.hour)\n#     df['dropoff_day_of_week']=df['dropoff_datetime'].apply(lambda x:x.weekday())\n#     df['dropoff_month']=df['dropoff_datetime'].apply(lambda x:x.month)\n#     df['dropoff_year']=df['dropoff_datetime'].apply(lambda x:x.year)\n    \n    return df", "execution_count": 20, "outputs": [{"output_type": "display_data", "data": {"text/plain": "VBox()", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "7dd30344545843e0b38b5b795e82ea08"}}, "metadata": {}}]}, {"metadata": {}, "cell_type": "markdown", "source": "## Write DF back to S3"}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "def writeDFtoS3(bucket, key, df):\n    # Write dataframe to buffer\n    csv_buffer = StringIO.StringIO()\n    df.to_csv(csv_buffer, index=False)\n\n    # Upload CSV to S3\n    s3 = boto3.client(\"s3\")\n    s3.put_object(Bucket=bucket, Key=key, Body=csv_buffer.getvalue())", "execution_count": 21, "outputs": [{"output_type": "display_data", "data": {"text/plain": "VBox()", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "b7d1a57379e14dad9ba97ccdfa7b4fdb"}}, "metadata": {}}]}, {"metadata": {}, "cell_type": "markdown", "source": "## Handler function data preprocessing"}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "def handler(filenames):\n    inputBucket = 'nyc-tlc'\n    outputBucket = 'dic-taxi-fare-prediction'\n    count = 0\n\n    for filename in filenames:\n        try:\n            df = readfile(\"s3n://{}/{}\".format(inputBucket, filename))\n            if not sanityCheck(set(df.columns)):\n                print(\"Sanity check failed\")\n            else:\n                print(\"Sanity check succeeded\")\n            if \"pulocationid\" not in df.columns:\n                continue\n            df = remove_nan_values(df)\n            if \"pickup_longitude\" in df.columns:\n                df = remove_invalid_lat_long(df)\n            df = remove_invalid_fare_trips(df)\n            df = generate_zipcode_columns(df)\n            df = fix_column_datatypes(df)\n            df = generate_datetime_features(df)\n            df['trip_time'] = df.apply(lambda row: (row.dropoff_datetime - row.pickup_datetime).seconds, axis=1)\n#             df['trip_time'] = (df['dropoff_datetime'].values - df['pickup_datetime'].values)\n#             df['trip_time'] = df['trip_time'].apply(lambda x:x.seconds)\n            df = df.reset_index(drop=True)\n            writeDFtoS3(outputBucket, filename, df)\n            count += 1\n#             print(str(filename)+\" is done processing -- df shape : \"+str(df.shape)+\"--- count : \"+str(count))\n        except:\n            continue\n        \n    return []", "execution_count": 22, "outputs": [{"output_type": "display_data", "data": {"text/plain": "VBox()", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "94c9dad17ad64bda8458fa095f69161f"}}, "metadata": {}}]}, {"metadata": {}, "cell_type": "markdown", "source": "## Read files names to be processed"}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "def getFilenames():\n    bucket = 'nyc-tlc'\n    prefix = \"trip data\"\n    trip_type = \"green_tripdata\"\n\n    s3 = boto3.client(\"s3\")\n    objects = s3.list_objects(Bucket=bucket, Prefix=prefix)\n    tripdata_files = []\n\n    if objects[\"ResponseMetadata\"] and objects[\"ResponseMetadata\"][\"HTTPStatusCode\"] == 200:\n        for obj in objects['Contents']:\n            if \"Key\" in obj and trip_type in obj[\"Key\"]:\n                tripdata_files.append(obj[\"Key\"])\n        print(\"Found {} under path s3://{}/{} for {}\".format(len(tripdata_files), bucket, prefix, trip_type))\n    else:\n        print(\"Could not read files under s3://{}/{}. Exiting.\".format(bucket, prefix))\n\n    return tripdata_files", "execution_count": 23, "outputs": [{"output_type": "display_data", "data": {"text/plain": "VBox()", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "8845c4b9e17146889ad6744b2bba7ba5"}}, "metadata": {}}]}, {"metadata": {}, "cell_type": "markdown", "source": "## Parallelize file processing "}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "st = datetime.now()", "execution_count": 24, "outputs": [{"output_type": "display_data", "data": {"text/plain": "VBox()", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "91f6dad019c1422283bdc1e83091bb17"}}, "metadata": {}}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "tripdata_files = getFilenames()\npartitions = 8 # 16\nrdd = sc.parallelize(tripdata_files[8:], 8) # 16 = number total cores of workers in the cluster\nrdd.mapPartitions(handler).collect()\n# handler(tripdata_files[:partitions])\n# print(list(tripdata_files))\n# rdd.getNumPartitions()\n# dir(rdd)\n# print(\"Default parallelism: {}\".format(sc.defaultParallelism))\n# print(\"Number of partitions: {}\".format(rdd.getNumPartitions()))\n# print(\"Partitioner: {}\".format(rdd.partitioner))\n# print(\"Partitions structure: {}\".format(rdd.glom().collect()))", "execution_count": 27, "outputs": [{"output_type": "display_data", "data": {"text/plain": "VBox()", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "05a850fdada348b08c3a89426ee023be"}}, "metadata": {}}, {"output_type": "stream", "text": "An error was encountered:\nError sending http request and maximum retry encountered.\n", "name": "stderr"}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "et = datetime.now()", "execution_count": 54, "outputs": [{"output_type": "display_data", "data": {"text/plain": "VBox()", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "a3abf93fa9e34b5b8405a1c9474593f2"}}, "metadata": {}}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "print((et-st).seconds)", "execution_count": 55, "outputs": [{"output_type": "display_data", "data": {"text/plain": "VBox()", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "124faf99ca7c471a980af5eae8b9d91e"}}, "metadata": {}}, {"output_type": "stream", "text": "75", "name": "stdout"}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "", "execution_count": null, "outputs": []}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "", "execution_count": null, "outputs": []}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "", "execution_count": null, "outputs": []}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "", "execution_count": null, "outputs": []}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "", "execution_count": null, "outputs": []}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "", "execution_count": null, "outputs": []}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "", "execution_count": null, "outputs": []}], "metadata": {"kernelspec": {"name": "pysparkkernel", "display_name": "PySpark", "language": ""}, "language_info": {"name": "pyspark", "mimetype": "text/x-python", "codemirror_mode": {"name": "python", "version": 2}, "pygments_lexer": "python2"}}, "nbformat": 4, "nbformat_minor": 2}