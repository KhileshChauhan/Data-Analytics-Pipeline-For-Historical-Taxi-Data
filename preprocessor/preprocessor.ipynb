{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to\n",
      "      ____              __\n",
      "     / __/__  ___ _____/ /__\n",
      "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
      "   /__ / .__/\\_,_/_/ /_/\\_\\   version 2.3.2\n",
      "      /_/\n",
      "\n",
      "Using Python version 2.7.15 (default, Oct  2 2018 11:42:04)\n",
      "SparkSession available as 'spark'.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "execfile(os.path.join(os.environ[\"SPARK_HOME\"], 'python/pyspark/shell.py'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract the column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'VendorID',\n",
       " u'lpep_pickup_datetime',\n",
       " u'lpep_dropoff_datetime',\n",
       " u'store_and_fwd_flag',\n",
       " u'RatecodeID',\n",
       " u'PULocationID',\n",
       " u'DOLocationID',\n",
       " u'passenger_count',\n",
       " u'trip_distance',\n",
       " u'fare_amount',\n",
       " u'extra',\n",
       " u'mta_tax',\n",
       " u'tip_amount',\n",
       " u'tolls_amount',\n",
       " u'ehail_fee',\n",
       " u'improvement_surcharge',\n",
       " u'total_amount',\n",
       " u'payment_type',\n",
       " u'trip_type']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datafile = \"green_tripdata_2018-06.csv\"\n",
    "with open(datafile) as f:\n",
    "    first_line = f.readline()\n",
    "col_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter out the useful columns from dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dolocationid': 6,\n",
       " 'dropoff_datetime': 2,\n",
       " 'fare_amount': 9,\n",
       " 'passenger_count': 7,\n",
       " 'pickup_datetime': 1,\n",
       " 'pulocationid': 5,\n",
       " 'trip_distance': 8}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "column_map = {}\n",
    "useful_columns = [\"pickup_longitude\", \"pickup_latitude\", \"dropoff_longitude\", \"dropoff_latitude\", \"pickup_datetime\",\"pickup_latitude\", \"pickup_longitude\",\"dropoff_datetime\",\"dolocationid\", \"pulocationid\", \"fare_amount\", \"trip_distance\", \"passenger_count\"]\n",
    "\n",
    "for i, c in enumerate(col_names):\n",
    "    c = c.lower()\n",
    "    for u in useful_columns:\n",
    "        if c in u or u in c:\n",
    "            column_map[u] = i\n",
    "column_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a dataframes from rdds. Cast them to appropriate type. Drop null valued rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------------------+-----------+---------------+-------------------+------------+-------------+\n",
      "|dolocationid|   dropoff_datetime|fare_amount|passenger_count|    pickup_datetime|pulocationid|trip_distance|\n",
      "+------------+-------------------+-----------+---------------+-------------------+------------+-------------+\n",
      "|          33|2018-06-01 00:36:13|        4.0|              5|2018-06-01 00:33:55|          66|         0.51|\n",
      "|          49|2018-06-01 00:49:46|        9.0|              5|2018-06-01 00:40:36|          25|         1.97|\n",
      "|          49|2018-06-01 01:02:58|        6.5|              5|2018-06-01 00:57:12|          61|          1.4|\n",
      "|          97|2018-06-01 00:16:27|        7.0|              1|2018-06-01 00:10:13|          49|         1.36|\n",
      "|         127|2018-06-01 00:52:06|       24.0|              1|2018-06-01 00:32:08|          75|          7.9|\n",
      "|         112|2018-06-01 00:27:44|       11.0|              2|2018-06-01 00:15:35|          36|          2.9|\n",
      "|          49|2018-06-01 00:57:26|        9.5|              1|2018-06-01 00:49:03|         256|          2.6|\n",
      "|          28|2018-06-01 01:38:02|       45.0|              2|2018-06-01 00:49:31|          25|        14.64|\n",
      "|         179|2018-06-01 00:22:05|        5.5|              1|2018-06-01 00:16:57|         179|          0.9|\n",
      "|         179|2018-06-01 00:46:15|        5.5|              2|2018-06-01 00:42:02|         179|          0.9|\n",
      "|         129|2018-06-01 00:22:47|        9.5|              1|2018-06-01 00:11:58|           7|          2.0|\n",
      "|          81|2018-06-01 00:23:29|       18.0|              1|2018-06-01 00:12:54|         212|         5.93|\n",
      "|          17|2018-06-01 00:28:08|        6.0|              2|2018-06-01 00:22:02|         256|          1.1|\n",
      "|         225|2018-06-01 00:52:17|       11.5|              1|2018-06-01 00:37:58|          80|          2.3|\n",
      "|         168|2018-06-01 00:11:05|        8.0|              1|2018-06-01 00:02:32|          41|         1.57|\n",
      "|         257|2018-06-01 01:11:18|       11.0|              5|2018-06-01 00:59:23|         106|         2.58|\n",
      "|         257|2018-06-01 00:25:46|       12.5|              1|2018-06-01 00:11:24|          33|         3.27|\n",
      "|         229|2018-06-01 00:19:42|       12.0|              1|2018-06-01 00:09:20|         226|         2.99|\n",
      "|         179|2018-06-01 00:53:05|        6.0|              1|2018-06-01 00:48:39|           7|         1.26|\n",
      "|         223|2018-06-01 01:03:32|        6.5|              1|2018-06-01 00:57:37|           7|         1.54|\n",
      "+------------+-------------------+-----------+---------------+-------------------+------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "from pyspark.sql.types import FloatType, IntegerType\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "rdd1 = sc.textFile(datafile).map(lambda line: line.split(\",\")).filter(lambda line: len(line)>1)\n",
    "\n",
    "if 'dolocationid' in column_map:\n",
    "    df = rdd1.map(lambda line: Row(pulocationid=line[column_map['pulocationid']], \n",
    "                              dolocationid=line[column_map['dolocationid']], \n",
    "                              pickup_datetime=line[column_map['pickup_datetime']], \n",
    "                              dropoff_datetime=line[column_map['dropoff_datetime']],\n",
    "                              trip_distance=line[column_map['trip_distance']], \n",
    "                              fare_amount=line[column_map['fare_amount']], \n",
    "                              passenger_count=line[column_map['passenger_count']])).toDF()\n",
    "    df = df.withColumn(\"dolocationid\", df[\"dolocationid\"].cast(IntegerType()))\n",
    "    df = df.withColumn(\"fare_amount\",df[\"fare_amount\"].cast(FloatType()))\n",
    "    df = df.withColumn(\"passenger_count\", df[\"passenger_count\"].cast(IntegerType()))\n",
    "    df = df.withColumn(\"pulocationid\", df[\"pulocationid\"].cast(IntegerType()))\n",
    "    df = df.withColumn(\"trip_distance\", df[\"trip_distance\"].cast(FloatType()))\n",
    "    \n",
    "    df = df.filter(col(\"dolocationid\").isNotNull() & col(\"dropoff_datetime\").isNotNull() & \\\n",
    "                   col(\"fare_amount\").isNotNull() & col(\"passenger_count\").isNotNull() & \\\n",
    "                   col(\"pulocationid\").isNotNull() & col(\"trip_distance\").isNotNull() & col(\"pickup_datetime\").isNotNull())\n",
    "\n",
    "else:\n",
    "    df = rdd1.map(lambda line: Row(pickup_longitude=line[column_map['pickup_longitude']], \n",
    "                                  pickup_latitude=line[column_map['pickup_latitude']], \n",
    "                                  dropoff_longitude=line[column_map['dropoff_longitude']], \n",
    "                                  dropoff_latitude=line[column_map['dropoff_latitude']], \n",
    "                                  pickup_datetime=line[column_map['pickup_datetime']], \n",
    "                                  dropoff_datetime=line[column_map['dropoff_datetime']], \n",
    "                                  trip_distance=line[column_map['trip_distance']], \n",
    "                                  fare_amount=line[column_map['fare_amount']], \n",
    "                                  passenger_count=line[column_map['passenger_count']])).toDF()\n",
    "    df = df.withColumn(\"dropoff_longitude\", df[\"dropoff_longitude\"].cast(FloatType()))\n",
    "    df = df.withColumn(\"dropoff_latitude\", df[\"dropoff_latitude\"].cast(FloatType()))\n",
    "    df = df.withColumn(\"fare_amount\",df[\"fare_amount\"].cast(FloatType()))\n",
    "    df = df.withColumn(\"passenger_count\", df[\"passenger_count\"].cast(IntegerType()))\n",
    "    df = df.withColumn(\"pickup_longitude\", df[\"pickup_longitude\"].cast(FloatType()))\n",
    "    df = df.withColumn(\"pickup_latitude\", df[\"pickup_latitude\"].cast(FloatType()))\n",
    "    df = df.withColumn(\"trip_distance\", df[\"trip_distance\"].cast(FloatType()))\n",
    "    df = df.filter(col(\"dropoff_longitude\").isNotNull() & col(\"dropoff_datetime\").isNotNull() & \\\n",
    "                   col(\"fare_amount\").isNotNull() & col(\"passenger_count\").isNotNull() & \\\n",
    "                   col(\"dropoff_longitude\").isNotNull() & col(\"trip_distance\").isNotNull() & \\\n",
    "                   col(\"pickup_latitude\").isNotNull() & col(\"pickup_longitude\").isNotNull() & \\\n",
    "                   col(\"pickup_datetime\").isNotNull())\n",
    "\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the pickup and dropoff zipcode for the trip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'dolocationid' in column_map:\n",
    "    import csv\n",
    "    from itertools import chain\n",
    "    from pyspark.sql.functions import create_map, lit\n",
    "    \n",
    "    mapFile = 'locationid_zipcode_map.csv'\n",
    "    reader = csv.reader(open(mapFile, 'r'))\n",
    "    zipcodeMap = {}\n",
    "    reader.next()\n",
    "    for row in reader:\n",
    "        try:\n",
    "            zipcodeMap[int(row[0])] = int(row[1])\n",
    "        except ValueError:\n",
    "            continue\n",
    "\n",
    "    mapping_expr = create_map([lit(x) for x in chain(*zipcodeMap.items())])\n",
    "    df = df.withColumn(\"dropoff_zipcode\", mapping_expr[df[\"dolocationid\"]])\n",
    "    df = df.withColumn(\"pickup_zipcode\", mapping_expr[df[\"pulocationid\"]])\n",
    "    df = df.drop(\"dolocationid\", \"pulocationid\")\n",
    "#     df['dropoff_zipcode'] = df.apply(lambda x: zipcodeMap(x['dolocationid']), axis=1)\n",
    "#     df['pickup_zipcode'] = df.apply(lambda x: zipcodeMap(x['pulocationid']), axis=1)\n",
    "else:\n",
    "    from uszipcode import SearchEngine\n",
    "    from uszipcode import Zipcode\n",
    "    def get_zip_code(lat,lng):\n",
    "        result = search.by_coordinates(lat, lng, radius=5, returns=1)\n",
    "        return result[0]\n",
    "                \n",
    "    df['dropoff_zipcode'] = df.apply(lambda x: zipcodeMap(x['dropoff_latitude'], x['dropoff_longitude']), axis=1)\n",
    "    df['pickup_zipcode'] = df.apply(lambda x: zipcodeMap(x['pickup_latitude'], x['pickup_longitude']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----------+---------------+-------------------+-------------+---------------+--------------+\n",
      "|   dropoff_datetime|fare_amount|passenger_count|    pickup_datetime|trip_distance|dropoff_zipcode|pickup_zipcode|\n",
      "+-------------------+-----------+---------------+-------------------+-------------+---------------+--------------+\n",
      "|2018-06-01 00:36:13|        4.0|              5|2018-06-01 00:33:55|         0.51|          11201|         11201|\n",
      "|2018-06-01 00:49:46|        9.0|              5|2018-06-01 00:40:36|         1.97|          11205|         11201|\n",
      "|2018-06-01 01:02:58|        6.5|              5|2018-06-01 00:57:12|          1.4|          11205|         11238|\n",
      "|2018-06-01 00:16:27|        7.0|              1|2018-06-01 00:10:13|         1.36|          11205|         11205|\n",
      "|2018-06-01 00:52:06|       24.0|              1|2018-06-01 00:32:08|          7.9|          10034|         10029|\n",
      "|2018-06-01 00:27:44|       11.0|              2|2018-06-01 00:15:35|          2.9|          11222|         11221|\n",
      "|2018-06-01 00:57:26|        9.5|              1|2018-06-01 00:49:03|          2.6|          11205|         10467|\n",
      "|2018-06-01 01:38:02|       45.0|              2|2018-06-01 00:49:31|        14.64|          11435|         11201|\n",
      "|2018-06-01 00:22:05|        5.5|              1|2018-06-01 00:16:57|          0.9|          11103|         11103|\n",
      "|2018-06-01 00:46:15|        5.5|              2|2018-06-01 00:42:02|          0.9|          11103|         11103|\n",
      "|2018-06-01 00:22:47|        9.5|              1|2018-06-01 00:11:58|          2.0|          11372|         11101|\n",
      "|2018-06-01 00:23:29|       18.0|              1|2018-06-01 00:12:54|         5.93|          10466|         10472|\n",
      "|2018-06-01 00:28:08|        6.0|              2|2018-06-01 00:22:02|          1.1|          10506|         10467|\n",
      "|2018-06-01 00:52:17|       11.5|              1|2018-06-01 00:37:58|          2.3|          11205|         11211|\n",
      "|2018-06-01 00:11:05|        8.0|              1|2018-06-01 00:02:32|         1.57|          10451|         10026|\n",
      "|2018-06-01 01:11:18|       11.0|              5|2018-06-01 00:59:23|         2.58|          11215|         11215|\n",
      "|2018-06-01 00:25:46|       12.5|              1|2018-06-01 00:11:24|         3.27|          11215|         11201|\n",
      "|2018-06-01 00:19:42|       12.0|              1|2018-06-01 00:09:20|         2.99|          10022|         11104|\n",
      "|2018-06-01 00:53:05|        6.0|              1|2018-06-01 00:48:39|         1.26|          11103|         11101|\n",
      "|2018-06-01 01:03:32|        6.5|              1|2018-06-01 00:57:37|         1.54|          11105|         11101|\n",
      "+-------------------+-----------+---------------+-------------------+-------------+---------------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dic",
   "language": "python",
   "name": "dic"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
