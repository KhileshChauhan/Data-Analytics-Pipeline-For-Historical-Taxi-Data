{"cells": [{"metadata": {}, "cell_type": "markdown", "source": "## Import packages"}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "import sys\nimport boto3 \nimport pandas as pd\nimport numpy as np\nimport s3fs\nimport StringIO", "execution_count": 1, "outputs": [{"output_type": "display_data", "data": {"text/plain": "VBox()", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "f3d5b56164b748ce980294ef1804ef1f"}}, "metadata": {}}, {"output_type": "stream", "text": "Starting Spark application\n", "name": "stdout"}, {"output_type": "display_data", "data": {"text/plain": "<IPython.core.display.HTML object>", "text/html": "<table>\n<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>5</td><td>application_1543806882658_0006</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-172-31-25-251.us-west-1.compute.internal:20888/proxy/application_1543806882658_0006/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-172-31-29-75.us-west-1.compute.internal:8042/node/containerlogs/container_1543806882658_0006_01_000001/livy\">Link</a></td><td>\u2714</td></tr></table>"}, "metadata": {}}, {"output_type": "stream", "text": "SparkSession available as 'spark'.\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "markdown", "source": "## Read file and extract a map of required columns"}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "def readfile(filename):\n    # datafile=\"s3n://nyc-tlc/trip data/green_tripdata_2018-06.csv\" # has area codes\n    # datafile=\"s3n://nyc-tlc/trip data/yellow_tripdata_2015-07.csv\" # has lat/long data\n    df = pd.read_csv(filename)\n    column_map = {}\n    useful_columns = [\"pickup_longitude\", \"pickup_latitude\", \"dropoff_longitude\", \"dropoff_latitude\", \"pickup_datetime\", \"dropoff_datetime\", \"dolocationid\", \"pulocationid\", \"fare_amount\", \"trip_distance\", \"passenger_count\"]\n    col_names = set(df.columns)\n    drop_columns = []\n    for c in col_names:\n        cl = c.lower()\n        for u in useful_columns:\n            if cl in u or u in cl:\n                column_map[c] = u\n        if c not in column_map:\n            drop_columns.append(c)\n    print(drop_columns)\n    df = df.drop(drop_columns, axis=1)\n\n    df.rename(columns=column_map, inplace=True)\n    return df", "execution_count": 2, "outputs": [{"output_type": "display_data", "data": {"text/plain": "VBox()", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "fd226170490b496a83017639f5fd3fc0"}}, "metadata": {}}]}, {"metadata": {}, "cell_type": "markdown", "source": "## Sanity check - all the required columns should be present"}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "def sanityCheck(columns):\n    required_columns = [\"pickup_datetime\", \"dropoff_datetime\", \"fare_amount\", \"trip_distance\", \"passenger_count\"]\n    for c in required_columns:\n        if c not in columns:\n            print(\"Required column {} not found in the data. Exiting.\".format(c))\n            return False\n\n    exit = False\n    for c in [\"pickup_longitude\", \"pickup_latitude\", \"dropoff_longitude\", \"dropoff_latitude\"]:\n        if c not in columns:\n            exit = True\n\n    if exit:\n        for c in [\"dolocationid\", \"pulocationid\"]:\n            if c not in columns:\n                print(\"Required columns for location not found in the data. Exiting.\".format(c))\n                return False\n\n    print(\"Sanity check complete. Data can be preprocessed.\")\n    return True\n", "execution_count": 3, "outputs": [{"output_type": "display_data", "data": {"text/plain": "VBox()", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "6b92bef2c81e4f3d8a47878e3ff84aa4"}}, "metadata": {}}]}, {"metadata": {}, "cell_type": "markdown", "source": "## Feature engineering - add zipcode"}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "def addZipCode(df, column_map):\n    if 'dolocationid' in column_map:\n        from itertools import chain\n        from pyspark.sql.functions import create_map, lit\n        # this map has been generated separately, stored here for ease of access\n        zipcodeMap = {1: 7114, 2: 11430, 3: 10469, 4: 10009, 5: 10309, 6: 10305, 7: 11101, 8: 11105, 9: 11358, 10: 11434, 11: 11214, 12: 10280, 13: 10280, 14: 11209, 15: 11359, 16: 11361, 17: 10506, 18: 10458, 19: 11426, 20: 14813, 21: 11206, 22: 11207, 23: 10314, 24: 12913, 25: 11201, 26: 11212, 27: 11697, 28: 11435, 29: 11235, 30: 11693, 31: 10453, 32: 10462, 33: 11201, 34: 11251, 35: 11212, 36: 11221, 37: 11237, 38: 11411, 39: 11234, 40: 11231, 41: 10026, 42: 10027, 43: 10019, 44: 10309, 45: 10013, 46: 10464, 47: 10457, 48: 10001, 49: 11205, 50: 10002, 51: 10475, 52: 11201, 53: 11356, 54: 11231, 55: 11224, 56: 11368, 57: 11368, 58: 10465, 59: 10457, 60: 10459, 61: 11238, 62: 11205, 63: 11208, 64: 11363, 65: 11201, 66: 11201, 67: 11228, 68: 10019, 69: 10451, 70: 11369, 71: 11203, 72: 11236, 73: 11355, 74: 10035, 75: 10029, 76: 10003, 77: 11207, 78: 10457, 79: 10211, 80: 11211, 81: 10466, 82: 11373, 83: 11378, 84: 10308, 85: 11226, 86: 11691, 87: 10005, 88: 10006, 89: 11226, 90: 10010, 91: 11239, 92: 11355, 93: 11368, 94: 10468, 95: 11375, 96: 11385, 97: 11205, 98: 11365, 99: 10312, 100: 10018, 101: 11004, 102: 11385, 103: 10012, 104: 10012, 105: 10012, 106: 11215, 107: 10016, 108: 11223, 109: 10308, 110: 10306, 111: 11232, 112: 11222, 113: 10012, 114: 10013, 115: 10301, 116: 10031, 117: 11692, 118: 10314, 119: 10452, 120: 10034, 121: 11366, 122: 11423, 123: 11229, 124: 11414, 125: 10006, 126: 10474, 127: 10034, 128: 10031, 129: 11372, 130: 11412, 131: 11423, 132: 11430, 133: 11218, 134: 11415, 135: 11367, 136: 10468, 137: 10016, 138: 11371, 139: 11413, 140: 10021, 141: 10065, 142: 10023, 143: 10024, 144: 10013, 145: 11101, 146: 11101, 147: 10459, 148: 10009, 149: 11201, 150: 11235, 151: 10025, 152: 10027, 153: 10463, 154: 11234, 155: 11234, 156: 10303, 157: 11378, 158: 10014, 159: 10456, 160: 11379, 161: 10018, 162: 10022, 163: 10018, 164: 10017, 165: 11230, 166: 10027, 167: 10456, 168: 10451, 169: 10453, 170: 10010, 171: 11354, 172: 10306, 173: 11368, 174: 10467, 175: 11364, 176: 10306, 177: 11233, 178: 11230, 179: 11103, 180: 11416, 181: 11215, 182: 10462, 183: 10461, 184: 10461, 185: 10461, 186: 10016, 187: 10302, 188: 11225, 189: 11238, 190: 11215, 191: 11427, 192: 11355, 193: 11101, 194: 10035, 195: 11231, 196: 11374, 197: 11418, 198: 11385, 199: 11370, 200: 10471, 201: 11694, 202: 10044, 203: 11422, 204: 10309, 205: 11412, 206: 11378, 207: 11370, 208: 10469, 209: 10013, 210: 11235, 211: 10013, 212: 10472, 213: 10472, 214: 10305, 215: 11435, 216: 11420, 217: 11221, 218: 11413, 219: 11413, 220: 10463, 221: 10304, 222: 11239, 223: 11105, 224: 10009, 225: 11205, 226: 11104, 227: 11211, 228: 11232, 229: 10022, 230: 10036, 231: 10007, 232: 10002, 233: 10022, 234: 10003, 235: 10453, 236: 10021, 237: 10028, 238: 10044, 239: 10065, 240: 10463, 241: 10463, 242: 10461, 243: 10032, 244: 10034, 245: 10310, 246: 10036, 247: 10451, 248: 10460, 249: 10014, 250: 10462, 251: 10314, 252: 11357, 253: 11357, 254: 10467, 255: 10467, 256: 10467, 257: 11215, 258: 11421, 259: 10470, 260: 11377, 261: 10048, 262: 10028, 263: 10028, 264: '', 265: ''}\n        mapping_expr = create_map([lit(x) for x in chain(*zipcodeMap.items())])\n        df = df.withColumn(\"dropoff_zipcode\", mapping_expr[df[\"dolocationid\"]])\n        df = df.withColumn(\"pickup_zipcode\", mapping_expr[df[\"pulocationid\"]])\n        df = df.drop(\"dolocationid\", \"pulocationid\")\n    else:\n        from uszipcode import SearchEngine\n        from uszipcode import Zipcode\n        from pyspark.sql.functions import udf, array\n        import numpy as np\n\n        search = SearchEngine(simple_zipcode=True)\n        def get_zip_code(latitude,longitude):\n            try:\n                search = SearchEngine(simple_zipcode=True)\n                result = search.by_coordinates(latitude, longitude, radius=5, returns=1)\n                return result[0].zipcode\n            except ValueError as e:\n                return 10001\n\n        for index, row in df.iterrows():\n            lat = row['pickup_latitude']\n            lng = row['pickup_longitude']\n            zipcode = get_zip_code(lat,lng)\n            df.at[index,'zipcode'] = zipcode.zipcode\n    return df", "execution_count": 4, "outputs": [{"output_type": "display_data", "data": {"text/plain": "VBox()", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "5acc76dbefab4372aa92ccffc89dc1f6"}}, "metadata": {}}]}, {"metadata": {}, "cell_type": "markdown", "source": "## Remove NaN values"}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "def remove_nan_values(df):\n    df = df.replace(to_replace='None', value=np.nan).dropna()\n    print(df.shape)\n    df = df[(df != 0).all(1)]\n    print(df.shape)\n    return df", "execution_count": 5, "outputs": [{"output_type": "display_data", "data": {"text/plain": "VBox()", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "888912626c9b4dfa80c472a2c837e262"}}, "metadata": {}}]}, {"metadata": {}, "cell_type": "markdown", "source": "## Remove invalid latitude/longitude"}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "def remove_invalid_lat_long(df):\n    max_lat = 40.917577\n    min_lat = 40.477399 \n    max_long = -73.700272 \n    min_long = -74.259090\n    loc_cols = [\"pickup_longitude\", \"pickup_latitude\", \"dropoff_longitude\", \"dropoff_latitude\"]\n    print loc_cols\n    lat_cols = []\n    long_cols = []\n    for column in loc_cols:\n        if \"latitude\" in column.lower():\n            lat_cols.append(column)\n        elif \"longitude\" in column.lower():\n            long_cols.append(column)\n\n    for col in lat_cols:\n        df = df.loc[(df[col] >= min_lat) & (df[col] <= max_lat)]\n        print(df.shape)\n\n    for col in long_cols:\n        df = df.loc[(df[col] >= min_long) & (df[col] <= max_long)]\n        print(df.shape)\n    \n    return df", "execution_count": 6, "outputs": [{"output_type": "display_data", "data": {"text/plain": "VBox()", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "d13e2ac4a07b4df9b0eed22b3a2c886a"}}, "metadata": {}}]}, {"metadata": {}, "cell_type": "markdown", "source": "## Remove trips with invalid fare"}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "def remove_invalid_fare_trips(df):\n    df = df.loc[(df['fare_amount'] >= 2.5)]\n    return df", "execution_count": 7, "outputs": [{"output_type": "display_data", "data": {"text/plain": "VBox()", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "765c75fac0a343858e96810bab7fb246"}}, "metadata": {}}]}, {"metadata": {}, "cell_type": "markdown", "source": "## Write DF back to S3"}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "def writeDFtoS3(bucket, key, df):\n    # Write dataframe to buffer\n    csv_buffer = StringIO.StringIO()\n    df.to_csv(csv_buffer, index=False)\n\n    # Upload CSV to S3\n    s3 = boto3.client(\"s3\")\n    s3.put_object(Bucket=bucket, Key=key, Body=csv_buffer.getvalue())", "execution_count": 8, "outputs": [{"output_type": "display_data", "data": {"text/plain": "VBox()", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "f8db8ac8fc27418b922e29fe8633f285"}}, "metadata": {}}]}, {"metadata": {}, "cell_type": "markdown", "source": "## Handler function date preprocessing"}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "def handler(filenames):\n    inputBucket = 'nyc-tlc'\n    outputBucket = 'dic-taxi-fare-prediction'\n    count = 0\n    for filename in filenames:\n        try:\n            df = readfile(\"s3n://{}/{}\".format(inputBucket, filename))\n            if not sanityCheck(set(df.columns)):\n                print(\"Sanity check failed\")\n            else:\n                print(\"Sanity check succeeded\")\n            if \"pickup_longitude\" in df.columns:\n                df = remove_invalid_lat_long(df)\n            df = remove_nan_values(df)\n            df = remove_invalid_fare_trips(df)\n            writeDFtoS3(outputBucket, filename, df)\n            count += 1\n        except: # until the lat/long logic is fixed\n            continue\n        \n    return [count]", "execution_count": 9, "outputs": [{"output_type": "display_data", "data": {"text/plain": "VBox()", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "ce47f19c793946f185f2aa24c4d76e8a"}}, "metadata": {}}]}, {"metadata": {}, "cell_type": "markdown", "source": "## Read files names to be processed"}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "def getFilenames():\n    bucket = 'nyc-tlc'\n    prefix = \"trip data\"\n    trip_type = \"green_tripdata\"\n\n    s3 = boto3.client(\"s3\")\n    objects = s3.list_objects(Bucket=bucket, Prefix=prefix)\n    tripdata_files = []\n\n    if objects[\"ResponseMetadata\"] and objects[\"ResponseMetadata\"][\"HTTPStatusCode\"] == 200:\n        for obj in objects['Contents']:\n            if \"Key\" in obj and trip_type in obj[\"Key\"]:\n                tripdata_files.append(obj[\"Key\"])\n        print(\"Found {} under path s3://{}/{} for {}\".format(len(tripdata_files), bucket, prefix, trip_type))\n    else:\n        print(\"Could not read files under s3://{}/{}. Exiting.\".format(bucket, prefix))\n\n    return tripdata_files", "execution_count": 10, "outputs": [{"output_type": "display_data", "data": {"text/plain": "VBox()", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "89ac54b923c7409aba109596715e4397"}}, "metadata": {}}]}, {"metadata": {}, "cell_type": "markdown", "source": "## Parallelize file processing "}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "tripdata_files = getFilenames()\nrdd = sc.parallelize(tripdata_files, 16) # 16 = number total cores of workers in the cluster\nrdd.mapPartitions(handler).count()\n# rdd.getNumPartitions()\n# dir(rdd)\n# print(\"Default parallelism: {}\".format(sc.defaultParallelism))\n# print(\"Number of partitions: {}\".format(rdd.getNumPartitions()))\n# print(\"Partitioner: {}\".format(rdd.partitioner))\n# print(\"Partitions structure: {}\".format(rdd.glom().collect()))\n", "execution_count": 11, "outputs": [{"output_type": "display_data", "data": {"text/plain": "VBox()", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "b8a7d0166f354ae4ad63e895902b4791"}}, "metadata": {}}, {"output_type": "stream", "text": "Found 59 under path s3://nyc-tlc/trip data for green_tripdata\n----------------------------------------\nException happened during processing of request from ('127.0.0.1', 55498)\n----------------------------------------\n16\nTraceback (most recent call last):\n  File \"/usr/lib64/python2.7/SocketServer.py\", line 290, in _handle_request_noblock\n    self.process_request(request, client_address)\n  File \"/usr/lib64/python2.7/SocketServer.py\", line 318, in process_request\n    self.finish_request(request, client_address)\n  File \"/usr/lib64/python2.7/SocketServer.py\", line 331, in finish_request\n    self.RequestHandlerClass(request, client_address, self)\n  File \"/usr/lib64/python2.7/SocketServer.py\", line 652, in __init__\n    self.handle()\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/accumulators.py\", line 263, in handle\n    poll(authenticate_and_accum_updates)\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/accumulators.py\", line 238, in poll\n    if func():\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/accumulators.py\", line 251, in authenticate_and_accum_updates\n    received_token = self.rfile.read(len(auth_token))\nTypeError: object of type 'NoneType' has no len()", "name": "stdout"}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "", "execution_count": null, "outputs": []}], "metadata": {"kernelspec": {"name": "pysparkkernel", "display_name": "PySpark", "language": ""}, "language_info": {"name": "pyspark", "mimetype": "text/x-python", "codemirror_mode": {"name": "python", "version": 2}, "pygments_lexer": "python2"}}, "nbformat": 4, "nbformat_minor": 2}