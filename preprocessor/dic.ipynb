{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import boto3 \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import s3fs\n",
    "import StringIO\n",
    "from uszipcode import SearchEngine\n",
    "from uszipcode import Zipcode\n",
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "zipcodeMap = {1: 7114, 2: 11430, 3: 10469, 4: 10009, 5: 10309, 6: 10305, 7: 11101, 8: 11105, 9: 11358, 10: 11434, 11: 11214, 12: 10280, 13: 10280, 14: 11209, 15: 11359, 16: 11361, 17: 10506, 18: 10458, 19: 11426, 20: 14813, 21: 11206, 22: 11207, 23: 10314, 24: 12913, 25: 11201, 26: 11212, 27: 11697, 28: 11435, 29: 11235, 30: 11693, 31: 10453, 32: 10462, 33: 11201, 34: 11251, 35: 11212, 36: 11221, 37: 11237, 38: 11411, 39: 11234, 40: 11231, 41: 10026, 42: 10027, 43: 10019, 44: 10309, 45: 10013, 46: 10464, 47: 10457, 48: 10001, 49: 11205, 50: 10002, 51: 10475, 52: 11201, 53: 11356, 54: 11231, 55: 11224, 56: 11368, 57: 11368, 58: 10465, 59: 10457, 60: 10459, 61: 11238, 62: 11205, 63: 11208, 64: 11363, 65: 11201, 66: 11201, 67: 11228, 68: 10019, 69: 10451, 70: 11369, 71: 11203, 72: 11236, 73: 11355, 74: 10035, 75: 10029, 76: 10003, 77: 11207, 78: 10457, 79: 10211, 80: 11211, 81: 10466, 82: 11373, 83: 11378, 84: 10308, 85: 11226, 86: 11691, 87: 10005, 88: 10006, 89: 11226, 90: 10010, 91: 11239, 92: 11355, 93: 11368, 94: 10468, 95: 11375, 96: 11385, 97: 11205, 98: 11365, 99: 10312, 100: 10018, 101: 11004, 102: 11385, 103: 10012, 104: 10012, 105: 10012, 106: 11215, 107: 10016, 108: 11223, 109: 10308, 110: 10306, 111: 11232, 112: 11222, 113: 10012, 114: 10013, 115: 10301, 116: 10031, 117: 11692, 118: 10314, 119: 10452, 120: 10034, 121: 11366, 122: 11423, 123: 11229, 124: 11414, 125: 10006, 126: 10474, 127: 10034, 128: 10031, 129: 11372, 130: 11412, 131: 11423, 132: 11430, 133: 11218, 134: 11415, 135: 11367, 136: 10468, 137: 10016, 138: 11371, 139: 11413, 140: 10021, 141: 10065, 142: 10023, 143: 10024, 144: 10013, 145: 11101, 146: 11101, 147: 10459, 148: 10009, 149: 11201, 150: 11235, 151: 10025, 152: 10027, 153: 10463, 154: 11234, 155: 11234, 156: 10303, 157: 11378, 158: 10014, 159: 10456, 160: 11379, 161: 10018, 162: 10022, 163: 10018, 164: 10017, 165: 11230, 166: 10027, 167: 10456, 168: 10451, 169: 10453, 170: 10010, 171: 11354, 172: 10306, 173: 11368, 174: 10467, 175: 11364, 176: 10306, 177: 11233, 178: 11230, 179: 11103, 180: 11416, 181: 11215, 182: 10462, 183: 10461, 184: 10461, 185: 10461, 186: 10016, 187: 10302, 188: 11225, 189: 11238, 190: 11215, 191: 11427, 192: 11355, 193: 11101, 194: 10035, 195: 11231, 196: 11374, 197: 11418, 198: 11385, 199: 11370, 200: 10471, 201: 11694, 202: 10044, 203: 11422, 204: 10309, 205: 11412, 206: 11378, 207: 11370, 208: 10469, 209: 10013, 210: 11235, 211: 10013, 212: 10472, 213: 10472, 214: 10305, 215: 11435, 216: 11420, 217: 11221, 218: 11413, 219: 11413, 220: 10463, 221: 10304, 222: 11239, 223: 11105, 224: 10009, 225: 11205, 226: 11104, 227: 11211, 228: 11232, 229: 10022, 230: 10036, 231: 10007, 232: 10002, 233: 10022, 234: 10003, 235: 10453, 236: 10021, 237: 10028, 238: 10044, 239: 10065, 240: 10463, 241: 10463, 242: 10461, 243: 10032, 244: 10034, 245: 10310, 246: 10036, 247: 10451, 248: 10460, 249: 10014, 250: 10462, 251: 10314, 252: 11357, 253: 11357, 254: 10467, 255: 10467, 256: 10467, 257: 11215, 258: 11421, 259: 10470, 260: 11377, 261: 10048, 262: 10028, 263: 10028, 264: '', 265: ''}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read file and extract a map of required columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readfile(filename, row_count=None):\n",
    "    # datafile=\"s3n://nyc-tlc/trip data/green_tripdata_2018-06.csv\" # has area codes\n",
    "    # datafile=\"s3n://nyc-tlc/trip data/yellow_tripdata_2015-07.csv\" # has lat/long data\n",
    "    df = pd.read_csv(filename)\n",
    "    dfcolumns = pd.read_csv(filename, nrows = 1)\n",
    "    ncols = len(dfcolumns.columns)\n",
    "    if row_count:\n",
    "        df = pd.read_csv(filename, header = None, sep= ',', \n",
    "                     skiprows = 1, usecols = list(range(ncols)),\n",
    "                     names = dfcolumns.columns, low_memory=False, nrows=row_count)\n",
    "    else:\n",
    "        df = pd.read_csv(filename, header = None, sep= ',', \n",
    "                         skiprows = 1, usecols = list(range(ncols)),\n",
    "                         names = dfcolumns.columns, low_memory=False)\n",
    "    \n",
    "    column_map = {}\n",
    "    useful_columns = [\"pickup_longitude\", \"pickup_latitude\", \"dropoff_longitude\", \"dropoff_latitude\", \"pickup_datetime\", \"dropoff_datetime\", \"dolocationid\", \"pulocationid\", \"fare_amount\", \"trip_distance\", \"passenger_count\"]\n",
    "    col_names = set(df.columns)\n",
    "    drop_columns = []\n",
    "    for c in col_names:\n",
    "        cl = c.lower()\n",
    "        for u in useful_columns:\n",
    "            if cl in u or u in cl:\n",
    "                column_map[c] = u\n",
    "        if c not in column_map:\n",
    "            drop_columns.append(c)\n",
    "    print(drop_columns)\n",
    "    df = df.drop(drop_columns, axis=1)\n",
    "\n",
    "    df.rename(columns=column_map, inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data_file(csv_path, row_count=None):\n",
    "    dfcolumns = pd.read_csv(csv_path, nrows = 1)\n",
    "    ncols = len(dfcolumns.columns)\n",
    "    if row_count:\n",
    "        df = pd.read_csv(csv_path, header = None, sep= ',', \n",
    "                     skiprows = 1, usecols = list(range(ncols)),\n",
    "                     names = dfcolumns.columns, low_memory=False, nrows=row_count)\n",
    "    else:\n",
    "        df = pd.read_csv(csv_path, header = None, sep= ',', \n",
    "                         skiprows = 1, usecols = list(range(ncols)),\n",
    "                         names = dfcolumns.columns, low_memory=False)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_columns(df, col_type=\"relevant\"):\n",
    "    cols = []\n",
    "    dfcols = list(df.columns)\n",
    "    if col_type == \"relevant\":\n",
    "        subs_to_check = ['time', 'location', 'passenger','distance', \n",
    "                         'ratecode', 'fare', \"longitude\", \"latitude\"]\n",
    "        for sub in subs_to_check:\n",
    "            for col in dfcols:\n",
    "                if sub.lower() in col.lower():\n",
    "                    cols.append(col)\n",
    "    \n",
    "    elif col_type == \"geolocation\":\n",
    "        subs_to_check = [\"location\", \"longitude\", \"latitude\"]\n",
    "        for sub in subs_to_check:\n",
    "            for col in dfcols:\n",
    "                if sub.lower() in col.lower():\n",
    "                    cols.append(col)\n",
    "    return cols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sanity check - all the required columns should be present"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sanityCheck(columns):\n",
    "    required_columns = [\"pickup_datetime\", \"dropoff_datetime\", \"fare_amount\", \"trip_distance\", \"passenger_count\"]\n",
    "    for c in required_columns:\n",
    "        if c not in columns:\n",
    "            print(\"Required column {} not found in the data. Exiting.\".format(c))\n",
    "            return False\n",
    "\n",
    "    exit = False\n",
    "    for c in [\"pickup_longitude\", \"pickup_latitude\", \"dropoff_longitude\", \"dropoff_latitude\"]:\n",
    "        if c not in columns:\n",
    "            exit = True\n",
    "\n",
    "    if exit:\n",
    "        for c in [\"dolocationid\", \"pulocationid\"]:\n",
    "            if c not in columns:\n",
    "                print(\"Required columns for location not found in the data. Exiting.\".format(c))\n",
    "                return False\n",
    "\n",
    "    print(\"Sanity check complete. Data can be preprocessed.\")\n",
    "    return True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature engineering - add zipcode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_zip_code(latitude,longitude):\n",
    "    try:\n",
    "        search = SearchEngine(simple_zipcode=True)\n",
    "        result = search.by_coordinates(latitude, longitude, radius=5, returns=1)\n",
    "        return result[0].zipcode\n",
    "    except ValueError as e:\n",
    "        return 10001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def applyZipCode(lat, long):\n",
    "    zipcode = get_zip_code(lat, long)\n",
    "    return zipcode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loc_id_zipcode(loc_id):\n",
    "    return zipcodeMap[loc_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_zipcode_columns(df):\n",
    "    if 'pickup_latitude' in df.columns:\n",
    "        df['pickup_zipcode'] = df.apply(lambda row: applyZipCode(row.pickup_latitude, row.pickup_longitude), axis=1)\n",
    "        df['dropoff_zipcode'] = df.apply(lambda row: applyZipCode(row.dropoff_latitude, row.dropoff_longitude), axis=1)\n",
    "        df.drop([\"pickup_longitude\",\"pickup_latitude\", \"dropoff_longitude\", \"dropoff_latitude\"], axis=1, inplace=True)\n",
    "    if 'dolocationid' in df.columns:\n",
    "        df['pickup_zipcode'] = df.apply(lambda row: get_loc_id_zipcode(row.pulocationid), axis=1)\n",
    "        df['dropoff_zipcode'] = df.apply(lambda row: get_loc_id_zipcode(row.dolocationid), axis=1)\n",
    "        df.drop([\"dolocationid\", \"pulocationid\"], axis=1, inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove NaN values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_nan_values(df):\n",
    "    df = df.replace(to_replace='None', value=np.nan).dropna()\n",
    "    print(df.shape)\n",
    "    df = df[(df != 0).all(1)]\n",
    "    print(df.shape)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove invalid latitude/longitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_invalid_lat_long(df):\n",
    "    max_lat = 40.917577\n",
    "    min_lat = 40.477399 \n",
    "    max_long = -73.700272 \n",
    "    min_long = -74.259090\n",
    "    loc_cols = [\"pickup_longitude\", \"pickup_latitude\", \"dropoff_longitude\", \"dropoff_latitude\"]\n",
    "    print(loc_cols)\n",
    "    lat_cols = []\n",
    "    long_cols = []\n",
    "    for column in loc_cols:\n",
    "        if \"latitude\" in column.lower():\n",
    "            lat_cols.append(column)\n",
    "        elif \"longitude\" in column.lower():\n",
    "            long_cols.append(column)\n",
    "\n",
    "    for col in lat_cols:\n",
    "        df = df.loc[(df[col] >= min_lat) & (df[col] <= max_lat)]\n",
    "        print(df.shape)\n",
    "\n",
    "    for col in long_cols:\n",
    "        df = df.loc[(df[col] >= min_long) & (df[col] <= max_long)]\n",
    "        print(df.shape)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove trips with invalid fare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_invalid_fare_trips(df):\n",
    "    df = df.loc[(df['fare_amount'] >= 2.5)]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fix Column Datatypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_column_datatypes(df):\n",
    "    date_columns = ['pickup_datetime', 'dropoff_datetime']\n",
    "    numeric_columns = ['passenger_count', 'trip_distance', 'fare_amount', 'pickup_zipcode', 'dropoff_zipcode']\n",
    "    df[date_columns] = df[date_columns].apply(pd.to_datetime)\n",
    "    df[numeric_columns] = df[numeric_columns].apply(pd.to_numeric)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Datetime Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_datetime_features(df):\n",
    "#     df['pickup_date']= df['pickup_datetime'].dt.date\n",
    "    df['pickup_day']=df['pickup_datetime'].apply(lambda x:x.day)\n",
    "    df['pickup_hour']=df['pickup_datetime'].apply(lambda x:x.hour)\n",
    "    df['pickup_day_of_week']=df['pickup_datetime'].apply(lambda x:x.weekday())\n",
    "    df['pickup_month']=df['pickup_datetime'].apply(lambda x:x.month)\n",
    "    df['pickup_year']=df['pickup_datetime'].apply(lambda x:x.year)\n",
    "    \n",
    "#     df['dropoff_date']= df['dropoff_datetime'].dt.date\n",
    "#     df['dropoff_day']=df['dropoff_datetime'].apply(lambda x:x.day)\n",
    "#     df['dropoff_hour']=df['dropoff_datetime'].apply(lambda x:x.hour)\n",
    "#     df['dropoff_day_of_week']=df['dropoff_datetime'].apply(lambda x:x.weekday())\n",
    "#     df['dropoff_month']=df['dropoff_datetime'].apply(lambda x:x.month)\n",
    "#     df['dropoff_year']=df['dropoff_datetime'].apply(lambda x:x.year)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write DF back to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def writeDFtoS3(bucket, key, df):\n",
    "    # Write dataframe to buffer\n",
    "    csv_buffer = StringIO.StringIO()\n",
    "    df.to_csv(csv_buffer, index=False)\n",
    "\n",
    "    # Upload CSV to S3\n",
    "    s3 = boto3.client(\"s3\")\n",
    "    s3.put_object(Bucket=bucket, Key=key, Body=csv_buffer.getvalue())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handler function date preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handler(filenames):\n",
    "    inputBucket = 'nyc-tlc'\n",
    "    outputBucket = 'dic-taxi-fare-prediction'\n",
    "    count = 0\n",
    "    for filename in filenames:\n",
    "        try:\n",
    "            df = readfile(\"s3n://{}/{}\".format(inputBucket, filename))\n",
    "            if not sanityCheck(set(df.columns)):\n",
    "                print(\"Sanity check failed\")\n",
    "            else:\n",
    "                print(\"Sanity check succeeded\")\n",
    "            df = remove_nan_values(df)\n",
    "            if \"pickup_longitude\" in df.columns:\n",
    "                df = remove_invalid_lat_long(df)\n",
    "            df = remove_invalid_fare_trips(df)\n",
    "            df = generate_zipcode_columns(df)\n",
    "            df = fix_column_datatypes(df)\n",
    "            df = generate_datetime_features(df)\n",
    "            df['trip_time'] = df.apply(lambda row: (row.dropoff_datetime - row.pickup_datetime).seconds, axis=1)\n",
    "            df = df.reset_index(drop=True)\n",
    "            writeDFtoS3(outputBucket, filename, df)\n",
    "            count += 1\n",
    "        except: # until the lat/long logic is fixed\n",
    "            continue\n",
    "        \n",
    "    return [count]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read files names to be processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getFilenames():\n",
    "    bucket = 'nyc-tlc'\n",
    "    prefix = \"trip data\"\n",
    "    trip_type = \"green_tripdata\"\n",
    "\n",
    "    s3 = boto3.client(\"s3\")\n",
    "    objects = s3.list_objects(Bucket=bucket, Prefix=prefix)\n",
    "    tripdata_files = []\n",
    "\n",
    "    if objects[\"ResponseMetadata\"] and objects[\"ResponseMetadata\"][\"HTTPStatusCode\"] == 200:\n",
    "        for obj in objects['Contents']:\n",
    "            if \"Key\" in obj and trip_type in obj[\"Key\"]:\n",
    "                tripdata_files.append(obj[\"Key\"])\n",
    "        print(\"Found {} under path s3://{}/{} for {}\".format(len(tripdata_files), bucket, prefix, trip_type))\n",
    "    else:\n",
    "        print(\"Could not read files under s3://{}/{}. Exiting.\".format(bucket, prefix))\n",
    "\n",
    "    return tripdata_files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parallelize file processing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tripdata_files = getFilenames()\n",
    "rdd = sc.parallelize(tripdata_files, 16) # 16 = number total cores of workers in the cluster\n",
    "rdd.mapPartitions(handler).count()\n",
    "# rdd.getNumPartitions()\n",
    "# dir(rdd)\n",
    "# print(\"Default parallelism: {}\".format(sc.defaultParallelism))\n",
    "# print(\"Number of partitions: {}\".format(rdd.getNumPartitions()))\n",
    "# print(\"Partitioner: {}\".format(rdd.partitioner))\n",
    "# print(\"Partitions structure: {}\".format(rdd.glom().collect()))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
