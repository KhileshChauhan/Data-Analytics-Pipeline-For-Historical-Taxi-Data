{"cells": [{"metadata": {}, "cell_type": "markdown", "source": "## Import packages"}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "# from pyspark.sql import Row\n# from pyspark.sql.types import FloatType, IntegerType\n# from pyspark.sql.functions import col\nimport sys\nimport boto3 \nimport pandas as pd\nimport numpy as np\nimport s3fs\nimport StringIO", "execution_count": 95, "outputs": [{"output_type": "display_data", "data": {"text/plain": "VBox()", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "0d4d913e42bc456c9e2d40061e840283"}}, "metadata": {}}]}, {"metadata": {}, "cell_type": "markdown", "source": "## Read file and extract a map of required columns"}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "def readfile(filename):\n    # datafile=\"s3n://nyc-tlc/trip data/green_tripdata_2018-06.csv\" # has area codes\n    # datafile=\"s3n://nyc-tlc/trip data/yellow_tripdata_2015-07.csv\" # has lat/long data\n    df = pd.read_csv(filename)\n    column_map = {}\n    useful_columns = [\"pickup_longitude\", \"pickup_latitude\", \"dropoff_longitude\", \"dropoff_latitude\", \"pickup_datetime\", \"dropoff_datetime\", \"dolocationid\", \"pulocationid\", \"fare_amount\", \"trip_distance\", \"passenger_count\"]\n    col_names = set(df.columns)\n    drop_columns = []\n    for c in col_names:\n        cl = c.lower()\n        for u in useful_columns:\n            if cl in u or u in cl:\n                column_map[c] = u\n        if c not in column_map:\n            drop_columns.append(c)\n    print(drop_columns)\n    df = df.drop(drop_columns, axis=1)\n\n    df.rename(columns=column_map, inplace=True)\n    return df", "execution_count": 96, "outputs": [{"output_type": "display_data", "data": {"text/plain": "VBox()", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "9c50d0ee48b24e88912563c9e86e064f"}}, "metadata": {}}]}, {"metadata": {}, "cell_type": "markdown", "source": "## Sanity check - all the required columns should be present"}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "def sanityCheck(columns):\n    required_columns = [\"pickup_datetime\", \"dropoff_datetime\", \"fare_amount\", \"trip_distance\", \"passenger_count\"]\n    for c in required_columns:\n        if c not in columns:\n            print(\"Required column {} not found in the data. Exiting.\".format(c))\n            return False\n\n    exit = False\n    for c in [\"pickup_longitude\", \"pickup_latitude\", \"dropoff_longitude\", \"dropoff_latitude\"]:\n        if c not in columns:\n            exit = True\n\n    if exit:\n        for c in [\"dolocationid\", \"pulocationid\"]:\n            if c not in columns:\n                print(\"Required columns for location not found in the data. Exiting.\".format(c))\n                return False\n\n    print(\"Sanity check complete. Data can be preprocessed.\")\n    return True\n", "execution_count": 97, "outputs": [{"output_type": "display_data", "data": {"text/plain": "VBox()", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "fa4d14f63271410c9c057a88eb78bf42"}}, "metadata": {}}]}, {"metadata": {}, "cell_type": "markdown", "source": "## Feature engineering - add zipcode"}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "def addZipCode(df, column_map):\n    if 'dolocationid' in column_map:\n        from itertools import chain\n        from pyspark.sql.functions import create_map, lit\n        # this map has been generated separately, stored here for ease of access\n        zipcodeMap = {1: 7114, 2: 11430, 3: 10469, 4: 10009, 5: 10309, 6: 10305, 7: 11101, 8: 11105, 9: 11358, 10: 11434, 11: 11214, 12: 10280, 13: 10280, 14: 11209, 15: 11359, 16: 11361, 17: 10506, 18: 10458, 19: 11426, 20: 14813, 21: 11206, 22: 11207, 23: 10314, 24: 12913, 25: 11201, 26: 11212, 27: 11697, 28: 11435, 29: 11235, 30: 11693, 31: 10453, 32: 10462, 33: 11201, 34: 11251, 35: 11212, 36: 11221, 37: 11237, 38: 11411, 39: 11234, 40: 11231, 41: 10026, 42: 10027, 43: 10019, 44: 10309, 45: 10013, 46: 10464, 47: 10457, 48: 10001, 49: 11205, 50: 10002, 51: 10475, 52: 11201, 53: 11356, 54: 11231, 55: 11224, 56: 11368, 57: 11368, 58: 10465, 59: 10457, 60: 10459, 61: 11238, 62: 11205, 63: 11208, 64: 11363, 65: 11201, 66: 11201, 67: 11228, 68: 10019, 69: 10451, 70: 11369, 71: 11203, 72: 11236, 73: 11355, 74: 10035, 75: 10029, 76: 10003, 77: 11207, 78: 10457, 79: 10211, 80: 11211, 81: 10466, 82: 11373, 83: 11378, 84: 10308, 85: 11226, 86: 11691, 87: 10005, 88: 10006, 89: 11226, 90: 10010, 91: 11239, 92: 11355, 93: 11368, 94: 10468, 95: 11375, 96: 11385, 97: 11205, 98: 11365, 99: 10312, 100: 10018, 101: 11004, 102: 11385, 103: 10012, 104: 10012, 105: 10012, 106: 11215, 107: 10016, 108: 11223, 109: 10308, 110: 10306, 111: 11232, 112: 11222, 113: 10012, 114: 10013, 115: 10301, 116: 10031, 117: 11692, 118: 10314, 119: 10452, 120: 10034, 121: 11366, 122: 11423, 123: 11229, 124: 11414, 125: 10006, 126: 10474, 127: 10034, 128: 10031, 129: 11372, 130: 11412, 131: 11423, 132: 11430, 133: 11218, 134: 11415, 135: 11367, 136: 10468, 137: 10016, 138: 11371, 139: 11413, 140: 10021, 141: 10065, 142: 10023, 143: 10024, 144: 10013, 145: 11101, 146: 11101, 147: 10459, 148: 10009, 149: 11201, 150: 11235, 151: 10025, 152: 10027, 153: 10463, 154: 11234, 155: 11234, 156: 10303, 157: 11378, 158: 10014, 159: 10456, 160: 11379, 161: 10018, 162: 10022, 163: 10018, 164: 10017, 165: 11230, 166: 10027, 167: 10456, 168: 10451, 169: 10453, 170: 10010, 171: 11354, 172: 10306, 173: 11368, 174: 10467, 175: 11364, 176: 10306, 177: 11233, 178: 11230, 179: 11103, 180: 11416, 181: 11215, 182: 10462, 183: 10461, 184: 10461, 185: 10461, 186: 10016, 187: 10302, 188: 11225, 189: 11238, 190: 11215, 191: 11427, 192: 11355, 193: 11101, 194: 10035, 195: 11231, 196: 11374, 197: 11418, 198: 11385, 199: 11370, 200: 10471, 201: 11694, 202: 10044, 203: 11422, 204: 10309, 205: 11412, 206: 11378, 207: 11370, 208: 10469, 209: 10013, 210: 11235, 211: 10013, 212: 10472, 213: 10472, 214: 10305, 215: 11435, 216: 11420, 217: 11221, 218: 11413, 219: 11413, 220: 10463, 221: 10304, 222: 11239, 223: 11105, 224: 10009, 225: 11205, 226: 11104, 227: 11211, 228: 11232, 229: 10022, 230: 10036, 231: 10007, 232: 10002, 233: 10022, 234: 10003, 235: 10453, 236: 10021, 237: 10028, 238: 10044, 239: 10065, 240: 10463, 241: 10463, 242: 10461, 243: 10032, 244: 10034, 245: 10310, 246: 10036, 247: 10451, 248: 10460, 249: 10014, 250: 10462, 251: 10314, 252: 11357, 253: 11357, 254: 10467, 255: 10467, 256: 10467, 257: 11215, 258: 11421, 259: 10470, 260: 11377, 261: 10048, 262: 10028, 263: 10028, 264: '', 265: ''}\n        mapping_expr = create_map([lit(x) for x in chain(*zipcodeMap.items())])\n        df = df.withColumn(\"dropoff_zipcode\", mapping_expr[df[\"dolocationid\"]])\n        df = df.withColumn(\"pickup_zipcode\", mapping_expr[df[\"pulocationid\"]])\n        df = df.drop(\"dolocationid\", \"pulocationid\")\n    else:\n        from uszipcode import SearchEngine\n        from uszipcode import Zipcode\n        from pyspark.sql.functions import udf, array\n        import numpy as np\n\n        search = SearchEngine(simple_zipcode=True)\n        def get_zip_code(latitude,longitude):\n            try:\n                search = SearchEngine(simple_zipcode=True)\n                result = search.by_coordinates(latitude, longitude, radius=5, returns=1)\n                return result[0].zipcode\n            except ValueError as e:\n                return 10001\n\n        for index, row in df.iterrows():\n            lat = row['pickup_latitude']\n            lng = row['pickup_longitude']\n            zipcode = get_zip_code(lat,lng)\n            df.at[index,'zipcode'] = zipcode.zipcode\n    return df", "execution_count": 99, "outputs": [{"output_type": "display_data", "data": {"text/plain": "VBox()", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "a9a749d9191c4364ad675de1e4f87682"}}, "metadata": {}}]}, {"metadata": {}, "cell_type": "markdown", "source": "## Remove NaN values"}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "def remove_nan_values(df):\n    df = df.replace(to_replace='None', value=np.nan).dropna()\n    print(df.shape)\n    df = df[(df != 0).all(1)]\n    print(df.shape)\n    return df", "execution_count": 100, "outputs": [{"output_type": "display_data", "data": {"text/plain": "VBox()", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "d3af3037b6954a3183ed81170038fd3f"}}, "metadata": {}}]}, {"metadata": {}, "cell_type": "markdown", "source": "## Remove invalid latitude/longitude"}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "def remove_invalid_lat_long(df):\n    max_lat = 40.917577\n    min_lat = 40.477399 \n    max_long = -73.700272 \n    min_long = -74.259090\n    loc_cols = [\"pickup_longitude\", \"pickup_latitude\", \"dropoff_longitude\", \"dropoff_latitude\"]\n    print loc_cols\n    lat_cols = []\n    long_cols = []\n    for column in loc_cols:\n        if \"latitude\" in column.lower():\n            lat_cols.append(column)\n        elif \"longitude\" in column.lower():\n            long_cols.append(column)\n\n    for col in lat_cols:\n        df = df.loc[(df[col] >= min_lat) & (df[col] <= max_lat)]\n        print(df.shape)\n\n    for col in long_cols:\n        df = df.loc[(df[col] >= min_long) & (df[col] <= max_long)]\n        print(df.shape)\n    \n    return df", "execution_count": 101, "outputs": [{"output_type": "display_data", "data": {"text/plain": "VBox()", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "ce249c12f8124ace9a6f40e90718a4ba"}}, "metadata": {}}]}, {"metadata": {}, "cell_type": "markdown", "source": "## Remove trips with invalid fare"}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "def remove_invalid_fare_trips(df):\n    df = df.loc[(df['fare_amount'] >= 2.5)]\n    return df", "execution_count": 102, "outputs": [{"output_type": "display_data", "data": {"text/plain": "VBox()", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "d4afcebee992455ba422898230bf3200"}}, "metadata": {}}]}, {"metadata": {}, "cell_type": "markdown", "source": "## Write DF back to S3"}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "def writeDFtoS3(bucket, key, df):\n    # Write dataframe to buffer\n    csv_buffer = StringIO.StringIO()\n    df.to_csv(csv_buffer, index=False)\n\n    # Upload CSV to S3\n    s3 = boto3.client(\"s3\")\n    s3.put_object(Bucket=bucket, Key=key, Body=csv_buffer.getvalue())", "execution_count": 103, "outputs": [{"output_type": "display_data", "data": {"text/plain": "VBox()", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "f7b48b7c0d164cb09a48c605dbda3fec"}}, "metadata": {}}]}, {"metadata": {}, "cell_type": "markdown", "source": "## Handler function date preprocessing"}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "def handler(filenames):\n    inputBucket = 'nyc-tlc'\n    outputBucket = 'dic-taxi-fare-prediction'\n    for filename in filenames:\n        try:\n            df = readfile(\"s3n://{}/{}\".format(inputBucket, filename))\n            if not sanityCheck(set(df.columns)):\n                print(\"Sanity check failed\")\n            else:\n                print(\"Sanity check succeeded\")\n            if \"pickup_longitude\" in df.columns:\n                df = remove_invalid_lat_long(df)\n            df = remove_nan_values(df)\n            df = remove_invalid_fare_trips(df)\n            writeDFtoS3(outputBucket, filename, df)\n        except: # until the lat/long logic is fixed\n            continue\n    return [1]\n        \n#         try:\n#             rdd, column_map = readfile(filename)\n#             if not sanityCheck(column_map):\n#                 continue\n#             df1 = createDF(rdd, column_map)\n#             df2 = addZipCode(df1, column_map)\n#             df3 = remove_nan_values(df2)\n#             if \"pickup_longitude\" in column_map:\n#                 df3 = remove_invalid_lat_long(df3)\n#             df4 = remove_invalid_fare_trips(df3)\n#             df4.write.save(outputDir, format='csv', header=True)\n#         except: # until the lat/long logic is fixed\n#             continue\n#     return [1]\n#     newRdd = df3.rdd.map(list)\n#     newRdd.saveAsTextFile(\"s3n://dic-taxi-fare-prediction/data\")\n\n# newRdd = handler(rdd)", "execution_count": 104, "outputs": [{"output_type": "display_data", "data": {"text/plain": "VBox()", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "998765fe4ec84837ab76cdfa9a5f6a2f"}}, "metadata": {}}]}, {"metadata": {}, "cell_type": "markdown", "source": "## Read files names to be processed"}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "def getFilenames():\n    bucket = 'nyc-tlc'\n    prefix = \"trip data\"\n    trip_type = \"green_tripdata\"\n\n    s3 = boto3.client(\"s3\")\n    objects = s3.list_objects(Bucket=bucket, Prefix=prefix)\n    tripdata_files = []\n    # print(objects[\"ResponseMetadata\"][\"HTTPStatusCode\"])\n    if objects[\"ResponseMetadata\"] and objects[\"ResponseMetadata\"][\"HTTPStatusCode\"] == 200:\n        for obj in objects['Contents']:\n            if \"Key\" in obj and trip_type in obj[\"Key\"]:\n                tripdata_files.append(obj[\"Key\"])\n        print(\"Found {} under path s3://{}/{} for {}\".format(len(tripdata_files), bucket, prefix, trip_type))\n    else:\n        print(\"Could not read files under s3://{}/{}. Exiting.\".format(bucket, prefix))\n\n    return tripdata_files", "execution_count": 105, "outputs": [{"output_type": "display_data", "data": {"text/plain": "VBox()", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "1c6fb833d7df474fa57dd2b4d8c36325"}}, "metadata": {}}]}, {"metadata": {}, "cell_type": "markdown", "source": "## Parallelize file processing "}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "# handler(rdd, column_map)\ndef test(filenames):\n    import socket\n    import boto3\n    import random\n    hostname = socket.gethostname()\n    bucket = 'atambol'\n    s3 = boto3.client(\"s3\")\n    for filename in filenames:\n        key = \"/\".join([hostname, filename, str(random.randint(1,10001))])\n        s3.put_object(Bucket=bucket, Key=key, Body=str.encode(\"h\"))\n        \n    return [1]\n        \n# sc.parallelize(tripdata_files, nPartitions).map(lambda element: test(element)).collect()\n''' this works on one node with orrect file count\nrdd = sc.parallelize(tripdata_files, 2)\n# rdd.count()\nrdd.mapPartitions(test).count()\n# dir(rdd)\n'''\n", "execution_count": 106, "outputs": [{"output_type": "display_data", "data": {"text/plain": "VBox()", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "92a9d95ff68c46d09aee8abb9f712a5a"}}, "metadata": {}}, {"output_type": "stream", "text": "' this works on one node with orrect file count\\nrdd = sc.parallelize(tripdata_files, 2)\\n# rdd.count()\\nrdd.mapPartitions(test).count()\\n# dir(rdd)\\n'", "name": "stdout"}]}, {"metadata": {"scrolled": true, "trusted": true}, "cell_type": "code", "source": "tripdata_files = getFilenames()\nrdd = sc.parallelize(tripdata_files, 16) # 16 = number total cores of workers in the cluster\nrdd.mapPartitions(handler).count()\n# rdd.getNumPartitions()\n# dir(rdd)\n# print(\"Default parallelism: {}\".format(sc.defaultParallelism))\n# print(\"Number of partitions: {}\".format(rdd.getNumPartitions()))\n# print(\"Partitioner: {}\".format(rdd.partitioner))\n# print(\"Partitions structure: {}\".format(rdd.glom().collect()))", "execution_count": 107, "outputs": [{"output_type": "display_data", "data": {"text/plain": "VBox()", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "0108bcea867d405b8145a3dc60cd360f"}}, "metadata": {}}, {"output_type": "stream", "text": "Found 59 under path s3://nyc-tlc/trip data for green_tripdata\n16", "name": "stdout"}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "", "execution_count": null, "outputs": []}], "metadata": {"kernelspec": {"name": "pysparkkernel", "display_name": "PySpark", "language": ""}, "language_info": {"name": "pyspark", "mimetype": "text/x-python", "codemirror_mode": {"name": "python", "version": 2}, "pygments_lexer": "python2"}}, "nbformat": 4, "nbformat_minor": 2}