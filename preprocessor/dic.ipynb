{"cells": [{"metadata": {}, "cell_type": "markdown", "source": "## Import packages"}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "from pyspark.sql import Row\nfrom pyspark.sql.types import FloatType, IntegerType\nfrom pyspark.sql.functions import col\nimport sys\nimport boto3 \nimport pandas as pd\nimport numpy as np", "execution_count": 1, "outputs": [{"output_type": "display_data", "data": {"text/plain": "VBox()", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "6d4ec149368c474cacdad293d8b76fbd"}}, "metadata": {}}, {"output_type": "stream", "text": "Starting Spark application\n", "name": "stdout"}, {"output_type": "display_data", "data": {"text/plain": "<IPython.core.display.HTML object>", "text/html": "<table>\n<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>2</td><td>application_1543806882658_0003</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-172-31-25-251.us-west-1.compute.internal:20888/proxy/application_1543806882658_0003/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-172-31-18-168.us-west-1.compute.internal:8042/node/containerlogs/container_1543806882658_0003_01_000001/livy\">Link</a></td><td>\u2714</td></tr></table>"}, "metadata": {}}, {"output_type": "stream", "text": "SparkSession available as 'spark'.\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "markdown", "source": "## Read file and extract a map of required columns"}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "def readfile(filename):\n    # datafile=\"s3n://nyc-tlc/trip data/green_tripdata_2018-06.csv\" # has area codes\n    # datafile=\"s3n://nyc-tlc/trip data/yellow_tripdata_2015-07.csv\" # has lat/long data\n    rdd = sc.textFile(datafile).map(lambda line: line.split(\",\")).filter(lambda line: len(line)>1)\n    col_names = rdd.take(1)[0]\n    column_map = {}\n    useful_columns = [\"pickup_longitude\", \"pickup_latitude\", \"dropoff_longitude\", \"dropoff_latitude\", \"pickup_datetime\", \"dropoff_datetime\", \"dolocationid\", \"pulocationid\", \"fare_amount\", \"trip_distance\", \"passenger_count\"]\n\n    for i, c in enumerate(col_names):\n        c = c.lower()\n        for u in useful_columns:\n            if c in u or u in c:\n                column_map[u] = i\n\n    print(\"Found attributes - {}\".format(column_map.keys()))\n    return rdd, column_map", "execution_count": 85, "outputs": [{"output_type": "display_data", "data": {"text/plain": "VBox()", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "0cbed53068ec477887edec7b7587c949"}}, "metadata": {}}]}, {"metadata": {}, "cell_type": "markdown", "source": "## Sanity check - all the required columns should be present"}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "def sanityCheck(column_map):\n    required_columns = [\"pickup_datetime\", \"dropoff_datetime\", \"fare_amount\", \"trip_distance\", \"passenger_count\"]\n    for c in required_columns:\n        if c not in column_map:\n            print(\"Required column {} not found in the data. Exiting.\".format(c))\n            return False\n\n    exit = False\n    for c in [\"pickup_longitude\", \"pickup_latitude\", \"dropoff_longitude\", \"dropoff_latitude\"]:\n        if c not in column_map:\n            exit = True\n\n    if exit:\n        for c in [\"dolocationid\", \"pulocationid\"]:\n            if c not in column_map:\n                print(\"Required columns for location not found in the data. Exiting.\".format(c))\n                return False\n\n    print(\"Sanity check complete. Data can be preprocessed.\")\n    return True\n", "execution_count": 86, "outputs": [{"output_type": "display_data", "data": {"text/plain": "VBox()", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "9eb834f1750141178f9fc000f6a21adb"}}, "metadata": {}}]}, {"metadata": {}, "cell_type": "markdown", "source": "## Create Dataframes from RDDs - cast them to proper type"}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "def createDF(rdd, column_map):\n    # remove headers\n    header = rdd.first()\n    rdd = rdd.filter(lambda line: line != header)\n    \n    # create df from rdd\n    if 'dolocationid' in column_map:\n        df = rdd.map(lambda line: Row(pulocationid=line[column_map['pulocationid']], \n                              dolocationid=line[column_map['dolocationid']], \n                              pickup_datetime=line[column_map['pickup_datetime']], \n                              dropoff_datetime=line[column_map['dropoff_datetime']],\n                              trip_distance=line[column_map['trip_distance']], \n                              fare_amount=line[column_map['fare_amount']], \n                              passenger_count=line[column_map['passenger_count']])).toDF()\n        \n        # assign correct type for columns\n        df = df.withColumn(\"dolocationid\", df[\"dolocationid\"].cast(IntegerType()))\n        df = df.withColumn(\"fare_amount\",df[\"fare_amount\"].cast(FloatType()))\n        df = df.withColumn(\"passenger_count\", df[\"passenger_count\"].cast(IntegerType()))\n        df = df.withColumn(\"pulocationid\", df[\"pulocationid\"].cast(IntegerType()))\n        df = df.withColumn(\"trip_distance\", df[\"trip_distance\"].cast(FloatType()))\n\n    else:\n        df = rdd.map(lambda line: Row(pickup_longitude=line[column_map['pickup_longitude']], \n                              pickup_latitude=line[column_map['pickup_latitude']], \n                              dropoff_longitude=line[column_map['dropoff_longitude']], \n                              dropoff_latitude=line[column_map['dropoff_latitude']], \n                              pickup_datetime=line[column_map['pickup_datetime']], \n                              dropoff_datetime=line[column_map['dropoff_datetime']], \n                              trip_distance=line[column_map['trip_distance']], \n                              fare_amount=line[column_map['fare_amount']], \n                              passenger_count=line[column_map['passenger_count']])).toDF()\n        df = df.withColumn(\"dropoff_longitude\", df[\"dropoff_longitude\"].cast(FloatType()))\n        df = df.withColumn(\"dropoff_latitude\", df[\"dropoff_latitude\"].cast(FloatType()))\n        df = df.withColumn(\"fare_amount\",df[\"fare_amount\"].cast(FloatType()))\n        df = df.withColumn(\"passenger_count\", df[\"passenger_count\"].cast(IntegerType()))\n        df = df.withColumn(\"pickup_longitude\", df[\"pickup_longitude\"].cast(FloatType()))\n        df = df.withColumn(\"pickup_latitude\", df[\"pickup_latitude\"].cast(FloatType()))\n        df = df.withColumn(\"trip_distance\", df[\"trip_distance\"].cast(FloatType()))\n    return df", "execution_count": 87, "outputs": [{"output_type": "display_data", "data": {"text/plain": "VBox()", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "05b3886df29243a4b6fef5627bae89b8"}}, "metadata": {}}]}, {"metadata": {}, "cell_type": "markdown", "source": "## Feature engineering - add zipcode"}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "def addZipCode(df, column_map):\n    if 'dolocationid' in column_map:\n        from itertools import chain\n        from pyspark.sql.functions import create_map, lit\n        # this map has been generated separately, stored here for ease of access\n        zipcodeMap = {1: 7114, 2: 11430, 3: 10469, 4: 10009, 5: 10309, 6: 10305, 7: 11101, 8: 11105, 9: 11358, 10: 11434, 11: 11214, 12: 10280, 13: 10280, 14: 11209, 15: 11359, 16: 11361, 17: 10506, 18: 10458, 19: 11426, 20: 14813, 21: 11206, 22: 11207, 23: 10314, 24: 12913, 25: 11201, 26: 11212, 27: 11697, 28: 11435, 29: 11235, 30: 11693, 31: 10453, 32: 10462, 33: 11201, 34: 11251, 35: 11212, 36: 11221, 37: 11237, 38: 11411, 39: 11234, 40: 11231, 41: 10026, 42: 10027, 43: 10019, 44: 10309, 45: 10013, 46: 10464, 47: 10457, 48: 10001, 49: 11205, 50: 10002, 51: 10475, 52: 11201, 53: 11356, 54: 11231, 55: 11224, 56: 11368, 57: 11368, 58: 10465, 59: 10457, 60: 10459, 61: 11238, 62: 11205, 63: 11208, 64: 11363, 65: 11201, 66: 11201, 67: 11228, 68: 10019, 69: 10451, 70: 11369, 71: 11203, 72: 11236, 73: 11355, 74: 10035, 75: 10029, 76: 10003, 77: 11207, 78: 10457, 79: 10211, 80: 11211, 81: 10466, 82: 11373, 83: 11378, 84: 10308, 85: 11226, 86: 11691, 87: 10005, 88: 10006, 89: 11226, 90: 10010, 91: 11239, 92: 11355, 93: 11368, 94: 10468, 95: 11375, 96: 11385, 97: 11205, 98: 11365, 99: 10312, 100: 10018, 101: 11004, 102: 11385, 103: 10012, 104: 10012, 105: 10012, 106: 11215, 107: 10016, 108: 11223, 109: 10308, 110: 10306, 111: 11232, 112: 11222, 113: 10012, 114: 10013, 115: 10301, 116: 10031, 117: 11692, 118: 10314, 119: 10452, 120: 10034, 121: 11366, 122: 11423, 123: 11229, 124: 11414, 125: 10006, 126: 10474, 127: 10034, 128: 10031, 129: 11372, 130: 11412, 131: 11423, 132: 11430, 133: 11218, 134: 11415, 135: 11367, 136: 10468, 137: 10016, 138: 11371, 139: 11413, 140: 10021, 141: 10065, 142: 10023, 143: 10024, 144: 10013, 145: 11101, 146: 11101, 147: 10459, 148: 10009, 149: 11201, 150: 11235, 151: 10025, 152: 10027, 153: 10463, 154: 11234, 155: 11234, 156: 10303, 157: 11378, 158: 10014, 159: 10456, 160: 11379, 161: 10018, 162: 10022, 163: 10018, 164: 10017, 165: 11230, 166: 10027, 167: 10456, 168: 10451, 169: 10453, 170: 10010, 171: 11354, 172: 10306, 173: 11368, 174: 10467, 175: 11364, 176: 10306, 177: 11233, 178: 11230, 179: 11103, 180: 11416, 181: 11215, 182: 10462, 183: 10461, 184: 10461, 185: 10461, 186: 10016, 187: 10302, 188: 11225, 189: 11238, 190: 11215, 191: 11427, 192: 11355, 193: 11101, 194: 10035, 195: 11231, 196: 11374, 197: 11418, 198: 11385, 199: 11370, 200: 10471, 201: 11694, 202: 10044, 203: 11422, 204: 10309, 205: 11412, 206: 11378, 207: 11370, 208: 10469, 209: 10013, 210: 11235, 211: 10013, 212: 10472, 213: 10472, 214: 10305, 215: 11435, 216: 11420, 217: 11221, 218: 11413, 219: 11413, 220: 10463, 221: 10304, 222: 11239, 223: 11105, 224: 10009, 225: 11205, 226: 11104, 227: 11211, 228: 11232, 229: 10022, 230: 10036, 231: 10007, 232: 10002, 233: 10022, 234: 10003, 235: 10453, 236: 10021, 237: 10028, 238: 10044, 239: 10065, 240: 10463, 241: 10463, 242: 10461, 243: 10032, 244: 10034, 245: 10310, 246: 10036, 247: 10451, 248: 10460, 249: 10014, 250: 10462, 251: 10314, 252: 11357, 253: 11357, 254: 10467, 255: 10467, 256: 10467, 257: 11215, 258: 11421, 259: 10470, 260: 11377, 261: 10048, 262: 10028, 263: 10028, 264: '', 265: ''}\n        mapping_expr = create_map([lit(x) for x in chain(*zipcodeMap.items())])\n        df = df.withColumn(\"dropoff_zipcode\", mapping_expr[df[\"dolocationid\"]])\n        df = df.withColumn(\"pickup_zipcode\", mapping_expr[df[\"pulocationid\"]])\n        df = df.drop(\"dolocationid\", \"pulocationid\")\n    else:\n        from uszipcode import SearchEngine\n        from uszipcode import Zipcode\n        from pyspark.sql.functions import udf, array\n        import numpy as np\n\n        search = SearchEngine(simple_zipcode=True)\n        def get_zip_code(latitude,longitude):\n            try:\n                search = SearchEngine(simple_zipcode=True)\n                result = search.by_coordinates(latitude, longitude, radius=5, returns=1)\n                return result[0].zipcode\n            except ValueError as e:\n                return 10001\n\n        for index, row in df.iterrows():\n            lat = row['pickup_latitude']\n            lng = row['pickup_longitude']\n            zipcode = get_zip_code(lat,lng)\n            df.at[index,'zipcode'] = zipcode.zipcode\n    return df", "execution_count": 88, "outputs": [{"output_type": "display_data", "data": {"text/plain": "VBox()", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "20dab4ff8ea14bd491bd89120347e077"}}, "metadata": {}}]}, {"metadata": {}, "cell_type": "markdown", "source": "## Remove NaN values"}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "def remove_nan_values(df):\n    num_columns = set(['dolocationid', 'fare_amount', 'passenger_count', 'pulocationid', 'trip_distance'])\n    for column in df.columns:\n        df = df.filter(col(column).isNotNull())\n        if column in num_columns:\n            df = df.filter(col(column) > 0.0)\n            print str(column)+ \"-->\" + str(df.count())\n    return df", "execution_count": 89, "outputs": [{"output_type": "display_data", "data": {"text/plain": "VBox()", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "b7bf97d8c8104a76a6606114bad03236"}}, "metadata": {}}]}, {"metadata": {}, "cell_type": "markdown", "source": "## Remove invalid latitude/longitude"}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "def remove_invalid_lat_long(df):\n    max_lat = 40.917577\n    min_lat = 40.477399 \n    max_long = -73.700272 \n    min_long = -74.259090\n    loc_cols = [\"pickup_longitude\", \"pickup_latitude\", \"dropoff_longitude\", \"dropoff_latitude\"]\n    print loc_cols\n    lat_cols = []\n    long_cols = []\n    for column in loc_cols:\n        if \"latitude\" in column.lower():\n            lat_cols.append(column)\n        elif \"longitude\" in column.lower():\n            long_cols.append(column)\n#     print lat_cols\n#     print long_cols\n            \n    for column in lat_cols:\n        df = df.filter(col(column).between(min_lat, max_lat))\n#         print \"lat : \"+ str(column) + \"-->\" + str(df.count())\n#         print (\"lat : {} ---> {}\".format(column, df.count()))\n    for column in long_cols:\n        df = df.filter(col(column).between(min_long, max_long))\n#         print (\"long : {} ---> {}\".format(column, df.count()))\n    return df", "execution_count": 90, "outputs": [{"output_type": "display_data", "data": {"text/plain": "VBox()", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "1d401f0cf4a8474f9cf54867ec18a6ee"}}, "metadata": {}}]}, {"metadata": {}, "cell_type": "markdown", "source": "## Remove trips with invalid fare"}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "def remove_invalid_fare_trips(df):\n    df = df.filter(col('fare_amount') >= 2.5)\n    return df", "execution_count": 91, "outputs": [{"output_type": "display_data", "data": {"text/plain": "VBox()", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "6b06454347bb4bb183606dee6abcdc09"}}, "metadata": {}}]}, {"metadata": {}, "cell_type": "markdown", "source": "## Handler function date preprocessing"}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "nPartitions = 2 # set this to the number of workers you have\noutputDir = \"s3n://dic-taxi-fare-prediction/data\"\ndef handler(filenames):\n    for filename in filenames:\n        try:\n            rdd, column_map = readfile(filename)\n            if not sanityCheck(column_map):\n                continue\n            df1 = createDF(rdd, column_map)\n            df2 = addZipCode(df1, column_map)\n            df3 = remove_nan_values(df2)\n            if \"pickup_longitude\" in column_map:\n                df3 = remove_invalid_lat_long(df3)\n            df4 = remove_invalid_fare_trips(df3)\n            df4.write.save(outputDir, format='csv', header=True)\n        except: # until the lat/long logic is fixed\n            continue\n    return [1]\n#     newRdd = df3.rdd.map(list)\n#     newRdd.saveAsTextFile(\"s3n://dic-taxi-fare-prediction/data\")\n\n# newRdd = handler(rdd)", "execution_count": 92, "outputs": [{"output_type": "display_data", "data": {"text/plain": "VBox()", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "8f504404bd6b49e09f57ec47f8d36bdf"}}, "metadata": {}}]}, {"metadata": {}, "cell_type": "markdown", "source": "## Read files names to be processed"}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "bucket = 'nyc-tlc'\nprefix = \"trip data\"\ntrip_type = \"green_tripdata\"\n\ns3 = boto3.client(\"s3\")\nobjects = s3.list_objects(Bucket=bucket, Prefix=prefix)\ntripdata_files = []\n# print(objects[\"ResponseMetadata\"][\"HTTPStatusCode\"])\nif objects[\"ResponseMetadata\"] and objects[\"ResponseMetadata\"][\"HTTPStatusCode\"] == 200:\n    for obj in objects['Contents']:\n        if \"Key\" in obj and trip_type in obj[\"Key\"]:\n            tripdata_files.append(obj[\"Key\"])\n    print(\"Found {} under path s3://{}/{} for {}\".format(len(tripdata_files), bucket, prefix, trip_type))\nelse:\n    print(\"Could not read files under s3://{}/{}. Exiting.\".format(bucket, prefix))\n    sys.exit(1)", "execution_count": 93, "outputs": [{"output_type": "display_data", "data": {"text/plain": "VBox()", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "b5d2c403b005460bb54c5e6cff9eb932"}}, "metadata": {}}, {"output_type": "stream", "text": "Found 59 under path s3://nyc-tlc/trip data for green_tripdata", "name": "stdout"}]}, {"metadata": {}, "cell_type": "markdown", "source": "## Parallelize file processing "}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "# handler(rdd, column_map)\ndef test(filenames):\n    import socket\n    import boto3\n    import random\n    hostname = socket.gethostname()\n    bucket = 'atambol'\n    s3 = boto3.client(\"s3\")\n    for filename in filenames:\n        key = \"/\".join([hostname, filename, str(random.randint(1,10001))])\n        s3.put_object(Bucket=bucket, Key=key, Body=str.encode(\"h\"))\n        \n    return [1]\n        \n# sc.parallelize(tripdata_files, nPartitions).map(lambda element: test(element)).collect()\n''' this works on one node with orrect file count\nrdd = sc.parallelize(tripdata_files, 2)\n# rdd.count()\nrdd.mapPartitions(test).count()\n# dir(rdd)\n'''\nrdd = sc.parallelize(tripdata_files, 16) # 16 = number total cores of workers in the cluster", "execution_count": 94, "outputs": [{"output_type": "display_data", "data": {"text/plain": "VBox()", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "de75a78322f045d08cacfd5ca5d17671"}}, "metadata": {}}]}, {"metadata": {"scrolled": true, "trusted": true}, "cell_type": "code", "source": "# rdd.count()\nrdd.mapPartitions(handler).count()\n# rdd.getNumPartitions()\n# dir(rdd)\n# print(\"Default parallelism: {}\".format(sc.defaultParallelism))\n# print(\"Number of partitions: {}\".format(rdd.getNumPartitions()))\n# print(\"Partitioner: {}\".format(rdd.partitioner))\n# print(\"Partitions structure: {}\".format(rdd.glom().collect()))", "execution_count": 95, "outputs": [{"output_type": "display_data", "data": {"text/plain": "VBox()", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "e58aab22784e4fa48a19bdb4705a1bf5"}}, "metadata": {}}, {"output_type": "stream", "text": "Could not serialize object: Exception: It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063.\nTraceback (most recent call last):\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/rdd.py\", line 1053, in count\n    return self.mapPartitions(lambda i: [sum(1 for _ in i)]).sum()\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/rdd.py\", line 1044, in sum\n    return self.mapPartitions(lambda x: [sum(x)]).fold(0, operator.add)\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/rdd.py\", line 915, in fold\n    vals = self.mapPartitions(func).collect()\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/rdd.py\", line 814, in collect\n    sock_info = self.ctx._jvm.PythonRDD.collectAndServe(self._jrdd.rdd())\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/rdd.py\", line 2472, in _jrdd\n    self._jrdd_deserializer, profiler)\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/rdd.py\", line 2405, in _wrap_function\n    pickled_command, broadcast_vars, env, includes = _prepare_for_python_RDD(sc, command)\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/rdd.py\", line 2391, in _prepare_for_python_RDD\n    pickled_command = ser.dumps(command)\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 575, in dumps\n    return cloudpickle.dumps(obj, 2)\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/cloudpickle.py\", line 918, in dumps\n    cp.dump(obj)\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/cloudpickle.py\", line 249, in dump\n    raise pickle.PicklingError(msg)\nPicklingError: Could not serialize object: Exception: It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063.\n\n", "name": "stderr"}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "tripdata_files\n", "execution_count": 59, "outputs": [{"output_type": "display_data", "data": {"text/plain": "VBox()", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "b6186337e8a7463fbf1357abaab714d6"}}, "metadata": {}}, {"output_type": "stream", "text": "An error was encountered:\nSession 1 did not reach idle status in time. Current status is busy.\n", "name": "stderr"}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "", "execution_count": null, "outputs": []}], "metadata": {"kernelspec": {"name": "pysparkkernel", "display_name": "PySpark", "language": ""}, "language_info": {"name": "pyspark", "mimetype": "text/x-python", "codemirror_mode": {"name": "python", "version": 2}, "pygments_lexer": "python2"}}, "nbformat": 4, "nbformat_minor": 2}